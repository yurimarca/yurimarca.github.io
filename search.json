[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Classify Time-Series Faults with Pytorch\n\n\n\n\n\n\nPyTorch\n\n\nMultilayer Perceptron\n\n\nNeural Networks\n\n\nPreprocessing\n\n\nDeep Learning\n\n\npython\n\n\n\nPreprocessing and MLP training to time-series data classification\n\n\n\n\n\nFeb 16, 2025\n\n\nYuri Marca\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an End-to-End ML Pipeline\n\n\n\n\n\n\nml pipelines\n\n\nreproducible experiments\n\n\nmlflow\n\n\nwandb\n\n\nhydra\n\n\npython\n\n\n\nProject development is part of the MLOps course from Udacity\n\n\n\n\n\nFeb 9, 2025\n\n\nYuri Marca\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "old_content/about.html",
    "href": "old_content/about.html",
    "title": "Yuri Marca’s Homepage",
    "section": "",
    "text": "Oct 2019 ~ Present – Warwick Business School, United Kingdom\n\nDoctor of Philosophy in Business and Management\n\nApr 2017 ~ Mar 2019 – Shinshu University, Japan\n\nMaster of Engineering in Electronics and Information System\n\nSept 2012 ~ Aug 2013 – Concordia University, Canada\n\nExchange Student, Electrical Engineering\n\nJan 2010 ~ Jul 2016 – Federal University of Technology - Parana (UTFPR), Brazil\n\nBachelor of Engineering in Electronics"
  },
  {
    "objectID": "old_content/about.html#education",
    "href": "old_content/about.html#education",
    "title": "Yuri Marca’s Homepage",
    "section": "",
    "text": "Oct 2019 ~ Present – Warwick Business School, United Kingdom\n\nDoctor of Philosophy in Business and Management\n\nApr 2017 ~ Mar 2019 – Shinshu University, Japan\n\nMaster of Engineering in Electronics and Information System\n\nSept 2012 ~ Aug 2013 – Concordia University, Canada\n\nExchange Student, Electrical Engineering\n\nJan 2010 ~ Jul 2016 – Federal University of Technology - Parana (UTFPR), Brazil\n\nBachelor of Engineering in Electronics"
  },
  {
    "objectID": "old_content/about.html#research-experience",
    "href": "old_content/about.html#research-experience",
    "title": "Yuri Marca’s Homepage",
    "section": "Research Experience",
    "text": "Research Experience\n\nApr 2017 ~ Mar 2019 – Shinshu University, Japan\n\nStudy the impact of difficult Pareto set topologies on the performance of multi-objective evolutionary algorithms (MOEA), and proposal of a new method to improve MOEA’s performance on such problems.\n\nMay 2013 ~ Aug 2013 – Concordia University, Canada\n\nShort research on the use of software filters to decrease signal interferences from eyes and muscles in electroencephalography.\n\nApr 2011 ~ Aug 2012 – Federal University of Technology - Parana (UTFPR), Brazil\n\nSupported a master’s student in business administration on the development of a Windows application that implemented the proposed method of his thesis.\nSupport to a medical research group on the development of an Android OS application for monitoring patients with diabetes."
  },
  {
    "objectID": "old_content/about.html#industry-experience",
    "href": "old_content/about.html#industry-experience",
    "title": "Yuri Marca’s Homepage",
    "section": "Industry Experience",
    "text": "Industry Experience\n\nOct 2014 ~ Dec 2015 – Internship at SSE Gridtech, Brazil\nJan 2016 ~ Aug 2016 – R&D Engineer at SSE Gridtech, Brazil\n\nEmbedded system development for long-distance communication radio as part of a smart metering solution."
  },
  {
    "objectID": "old_content/about.html#award-and-recognition",
    "href": "old_content/about.html#award-and-recognition",
    "title": "Yuri Marca’s Homepage",
    "section": "Award and Recognition",
    "text": "Award and Recognition\n\nBest Student Paper Award, International Conference on Evolutionary Multi-Criterion Optimization (EMO) 2019.\nYoung Researcher Award, IEEE Computational Intelligence Society Japan Chapter, Japanese Symposium on Evolutionary Computation 2018.\nYoung Research Paper Award, IEICE Shin-etsu 2018 Branch IEEE Session."
  },
  {
    "objectID": "old_content/about.html#technical-expertise",
    "href": "old_content/about.html#technical-expertise",
    "title": "Yuri Marca’s Homepage",
    "section": "Technical Expertise",
    "text": "Technical Expertise\nProgramming Languages: Python, Assembly, C, C++, Java, Bash (Unix Shell)\nEnvironment Tools: MATLAB, R Studio, Android Studio, OrCAD, Quartus II"
  },
  {
    "objectID": "old_content/index.html",
    "href": "old_content/index.html",
    "title": "Yuri Marca’s Homepage",
    "section": "",
    "text": "I am a PhD Student in the Operational Research group of Warwick Business School, U.K. I have earned my Master degree in Electronic and Information Engineering from Shinshu University, Japan, and my Bachelor degree in Electronic Engineering from Federal University of Technology – Paraná (UTFPR), Brazil. During my undergraduate studies, I studied one year at Concordia University, Canada as an exchange student participant of the program Science Without Borders. My research interests include Evolutionary Computation, Multi-objective Optimization, and Operational Research."
  },
  {
    "objectID": "old_content/index.html#summary",
    "href": "old_content/index.html#summary",
    "title": "Yuri Marca’s Homepage",
    "section": "",
    "text": "I am a PhD Student in the Operational Research group of Warwick Business School, U.K. I have earned my Master degree in Electronic and Information Engineering from Shinshu University, Japan, and my Bachelor degree in Electronic Engineering from Federal University of Technology – Paraná (UTFPR), Brazil. During my undergraduate studies, I studied one year at Concordia University, Canada as an exchange student participant of the program Science Without Borders. My research interests include Evolutionary Computation, Multi-objective Optimization, and Operational Research."
  },
  {
    "objectID": "old_content/index.html#news",
    "href": "old_content/index.html#news",
    "title": "Yuri Marca’s Homepage",
    "section": "NEWS",
    "text": "NEWS\n\n2019 November: Participant of the COST Action Training school CA15140 at University of Coimbra, Portugal.\n2019 October: I have started the PhD course at Warwick Business School, United Kingdom.\n2019 July: Our paper has been accepted to be published on Japan Evolutionary Computation Society’s Journal.\n\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, K. Tanaka: MOEA with Cubic Interpolation on Bi-objective Problems with Difficult Pareto Set Topology\n\n2019 March: I have graduated from Shinshu University, Japan.\n\nThesis’ title: Interpolation Model Based Evolutionary Algorithm to Solve Multi-objective Optimization Problems with Difficult Pareto Set Topology.\n\n2019 March: I received the Best Student Paper Award at EMO2019 for the paper:\n\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, K. Tanaka: Approximating Pareto set topology by cubic interpolation on bi-objective problems.\n\n2018 December: I received the Young Researcher Award from JPNSEC 2018 Symposium on Evolutionary Computation for the paper:\n\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka.: NSGA-II with Spline Interpolation on Bi-objective Problems with Difficult Pareto Set Topology.\n\n\nPast activities"
  },
  {
    "objectID": "old_content/index.html#contact",
    "href": "old_content/index.html#contact",
    "title": "Yuri Marca’s Homepage",
    "section": "Contact",
    "text": "Contact\n\nyurimarca [at] g m a i l [dot] c o m"
  },
  {
    "objectID": "old_content/index.html#online-cvs",
    "href": "old_content/index.html#online-cvs",
    "title": "Yuri Marca’s Homepage",
    "section": "Online CVs",
    "text": "Online CVs\n\nGoogle Scholar\nLinkedIn\nLattes Platform – Brazilian government’s research registry"
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "",
    "text": "Predictive maintenance and fault detection are critical in industrial applications to prevent severe equipment damage, reduce costly downtime, and ensure worker safety. This blog post explores how to preprocess time-series data and train a Multilayer Perceptron (MLP) using PyTorch to classify faults efficiently. For this project, we used the MAFAULDA dataset, which is composed of multivariate time-series acquired by sensors on a Machinery Fault Simulator (MFS). All the code presented in this post is accessible in my github repository.\n\n\n\nMultilayer Perceptron (MLP) illustration\n\n\nThese are the steps covered in the project:\n\nExploratory Data Analysis (EDA)\nPreprocessing/Feature Engineering\n\nDownsampling & Rolling Mean\nData Transformation & Visualization (t-SNE)\n\nBuilding a Multi-Layer Perceptron (MLP) with PyTorch\n\nCustom Dataset Class\nModel Architecture & Training\n\nEvaluation Metrics (Accuracy, F1-score, Precision, Recall, AUC-ROC)\n\nLet’s dive in!\n\n\n\n\n\nWe are dealing with vibration and microphone signals recorded from a motor simulator under two conditions:\n1. Normal\n2. Imbalance (faulty)\nThere are other types of faulty data present in the MAFAULDA’s website, but we only consider normal and imbalance (6g) datasets. After donwloading the data, we have this folder structure:\ndata\n├── imbalance\n│   └── imbalance\n│       ├── 10g\n│       ├── 15g\n│       ├── 20g\n│       ├── 25g\n│       ├── 30g\n│       ├── 35g\n│       └── 6g\n└── normal\n    └── normal\nAt each labeled folder presented above, there are multiple recordings stored in CSV files. Each recording is about 5 seconds at a 50 kHz sampling rate, resulting in 250,000 samples per sensor. The dataset includes eight features:\n\ntachometer\nunderhang_axial\nunderhang_radiale\nunderhang_tangential\noverhang_axial\noverhang_radiale\noverhang_tangential\nmicrophone\n\nThe first step is to get a sense of data distributions, missing values, basic statistics, etc. In EDA.ipynb, we use ydata-profiling package for a quickly review of how data looklike:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ydata_profiling import ProfileReport\n\n# Loading a CSV file for normal condition\ncol_names = [\n    'tachometer', 'underhang_axial', 'underhang_radiale', 'underhang_tangential',\n    'overhang_axial', 'overhang_radiale', 'overhang_tangential', 'microphone'\n]\nnormal_df = pd.read_csv(\"path/to/some_normal.csv\", names=col_names)\n\n# Generate HTML report (ydata-profiling)\nprofile = ProfileReport(normal_df, title=\"Normal Data\")\nprofile.to_file(\"normal_data_report.html\")\nThe generated report normal_data_report.html reveals distributions and correlations. This helps us spot which features might be redundant or highly correlated. For instance, you might notice that microphone and underhang_radiale have high correlation, which could inform feature selection later. Also, we noticed how the features are normally distributed, which could be lead to a smoother convergence when training models using Gradient descent. Besides being normally distributed, standardizing features (zero mean, unit variance) remains an important aspect of preprocessing.\n\n\n\nTo visualize how signal data changes over time, we can create quick plots of 50,000 samples (1 second snippet) from each class:\ndef plot_timeseries(df, columns, n_samples=50000):\n    plt.figure(figsize=(12, len(columns)))\n    for i, col in enumerate(columns, 1):\n        plt.subplot(len(columns), 1, i)\n        plt.plot(df[col].values[:n_samples])\n        plt.title(f\"Time Series of {col}\")\n    plt.tight_layout()\n    plt.show()\n\nplot_timeseries(normal_df, columns=normal_df.columns)\n\n\n\nNormal machinery dataset\n\n\n\n\n\n\n\nHigh-frequency time-series data can be noisy and large, so feature engineering becomes crucial. Two primary transformations were used in FeatureEngineering.ipynb and ultimately in the main pipeline:\n\nDownsampling: Consolidates raw data by averaging every b samples. In the repository, b=2500 was used, reducing 250,000 samples per file to just 100 samples.\n\nRolling Mean: Applies a moving average (with a given window size) to smooth abrupt fluctuations and incorporate temporal context into each feature.\n\nIn FeatureEngineering.ipynb, you’ll see:\ndef downSampler(data, b):\n    \"\"\"\n    Downsamples the given DataFrame by averaging every 'b' rows.\n    \"\"\"\n    return data.groupby(data.index // b).mean().reset_index(drop=True)\n\n\ndef rolling_mean_data(df, window_size=100, columns=None):\n    \"\"\"\n    Applies a rolling mean transformation to specified columns.\n    \"\"\"\n    if columns is None:\n        columns = df.select_dtypes(include=[np.number]).columns\n\n    df_copy = df.copy()\n    df_copy[columns] = df_copy[columns].rolling(window=window_size, min_periods=1).mean()\n    return df_copy\n\n# Usage:\nnormal_df = downSampler(normal_df, 2500)\nnormal_df = rolling_mean_data(normal_df, window_size=100)\nAfter applying both transformation, the behaviour of each feature across the time is presented below:\n\n\n\nNormal data after feature engineering transformations\n\n\n\n\nAfter feature transformations, we often test whether the classes (normal vs. imbalance) are more distinguishable, and we can visually check this using dimensionality reduction methods such as Principal component analysis for linear transformation or t-distributed Stochastic Neighbor Embedding for non-linear transformation. In this case, we apply t-SNE as it is capable of dealing with non-linear transformation.\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n\ndef plot_tsne(df, label_column='label'):\n    features = df.select_dtypes(include=[np.number]).drop(columns=[label_column], errors='ignore')\n    features_scaled = StandardScaler().fit_transform(features)\n   \n    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n    tsne_results = tsne.fit_transform(features_scaled)\n\n    # Plot\n    plt.figure(figsize=(8, 6))\n    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=df[label_column], cmap=\"viridis\", alpha=0.7)\n    plt.title(\"t-SNE Visualization\")\n    plt.show()\nThe following t-SNE plot clearly shows how both classes can be distinguished visually. This visualization gives us confidence that there is a non-linear transformation capable of producing a rule that will correctly classify the binary time-series dataset.\n\n\n\nt-SNE for visualizing distribution of binary classes\n\n\n\n\n\n\n\nWhile more sophisticated sequence models (e.g., LSTM, 1D CNNs) are more relevant for time-series data, a Multi-Layer Perceptron is a basic architecture that does not intrinsically include temporal dependencies. Since we have applied feature engineering capable of introducing basic temporal effects to features, the MLP might be sufficient for correctly classify this dataset. In general, if sequential dependencies matter, alternatives like RNN, LSTM, 1-D CNN, or Transformers may perform better, whereas MLP is effective for feature-based classification when raw time series is converted into useful representations through feature engineering.\n\n\nIn PyTorch, we create a custom Dataset class to handle how features and labels are fed to the model:\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass MachineryDataset(Dataset):\n    def __init__(self, data, label_column='label'):\n        self.labels = data[label_column].values.astype(np.float32)\n        self.features = data.drop(columns=[label_column, 'time'], errors='ignore').values.astype(np.float32)\n       \n    def __len__(self):\n        return len(self.features)\n   \n    def __getitem__(self, idx):\n        x = self.features[idx]\n        y = self.labels[idx]\n        return x, y\n\n__getitem__: Returns a single sample (features, label).\n__len__: Provides the total length of the dataset.\n\nWe also build a DataLoader object that batches the data and shuffles it during training:\ntrain_dataset = MachineryDataset(all_data, label_column='label')\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n\n\nA simple feed-forward neural network can be built using fully connected layers (nn.Linear):\nimport torch.nn as nn\n\nclass TimeSeriesMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, n_layers, dropout_prob=0.3):\n        super(TimeSeriesMLP, self).__init__()\n        layers = []\n        layers.append(nn.Linear(input_dim, hidden_dim))\n        layers.append(nn.ReLU())\n        layers.append(nn.Dropout(dropout_prob))\n\n        for _ in range(n_layers - 1):\n            layers.append(nn.Linear(hidden_dim, hidden_dim))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(dropout_prob))\n\n        layers.append(nn.Linear(hidden_dim, 1))\n        layers.append(nn.Sigmoid())\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\nHidden Layers and Activation:\nThe model consists of n_layers hidden layers, each applying a linear transformation (nn.Linear), followed by ReLU activation (nn.ReLU()).\nReLU (Rectified Linear Unit) is used because it helps mitigate the vanishing gradient problem and accelerates training.\nDropout Regularization:\nA dropout layer (nn.Dropout(dropout_prob)) is applied after each hidden layer to reduce overfitting.\nDropout randomly disables a fraction of neurons during training, forcing the model to learn more robust features.\nOutput Layer and Activation:\nThe final layer maps the last hidden representation to a single output neuron using nn.Linear(hidden_dim, 1).\nSigmoid activation (nn.Sigmoid()) is applied to ensure the output is in the range [0, 1], making it suitable for binary classification.\n\n\n\n\nThe train_model function in src/main.py file trains the MLP using a binary classification approach, tracking both loss and accuracy.\nimport torch.optim as optim\nimport torch\n\ndef train_model(model, train_loader, val_loader, epochs=50, lr=0.0005):\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n   \n    history = {\n        'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []\n    }\n   \n    for epoch in range(epochs):\n        model.train()  # Enable training mode\n        epoch_train_loss = 0\n        correct_train = 0\n        total_train = 0\n       \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs).squeeze()  # Forward pass\n            loss = criterion(outputs, targets)  # Compute loss\n            loss.backward()  # Backpropagation\n            optimizer.step()  # Update weights\n           \n            epoch_train_loss += loss.item()\n            preds = (outputs &gt; 0.5).float()  # Convert probabilities to binary predictions\n            correct_train += (preds == targets).sum().item()\n            total_train += targets.size(0)\n       \n        train_accuracy = correct_train / total_train  # Compute training accuracy\n       \n        model.eval()  # Enable evaluation mode\n        epoch_val_loss = 0\n        correct_val = 0\n        total_val = 0\n       \n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                outputs = model(inputs).squeeze()\n                val_loss = criterion(outputs, targets)\n                epoch_val_loss += val_loss.item()\n               \n                preds = (outputs &gt; 0.5).float()\n                correct_val += (preds == targets).sum().item()\n                total_val += targets.size(0)\n       \n        val_accuracy = correct_val / total_val  # Compute validation accuracy\n       \n        # Store metrics for analysis\n        history['train_loss'].append(epoch_train_loss / len(train_loader))\n        history['val_loss'].append(epoch_val_loss / len(val_loader))\n        history['train_acc'].append(train_accuracy)\n        history['val_acc'].append(val_accuracy)\n       \n        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {epoch_val_loss / len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n   \n    return history\n\nBinary Cross-Entropy Loss (nn.BCELoss()):\n\nSuitable for binary classification, where the target labels are 0 or 1.\n\nAdam Optimizer (optim.Adam()):\n\nAdaptive learning rate for better convergence.\n\nAccuracy Tracking:\n\nUses thresholding (outputs &gt; 0.5) to determine class predictions.\nCompares predictions to true labels (targets) to compute accuracy.\n\nTraining & Validation Loss History:\n\nLogs loss and accuracy at each epoch for performance monitoring.\n\n\n\n\n\n\n\nFinally, in src/main.py we orchestrate the entire workflow: 1. Data Ingestion & Labeling\n2. Feature Engineering (Downsampling, Rolling Mean, StandardScaler)\n3. Splitting (Time-series split into train/val/test sets)\n4. MLP Training\n5. Evaluation: Accuracy, F1, Precision, Recall, AUC-ROC\nBelow is a condensed snippet showing the pipeline’s main logic:\n# Load normal and imbalance data\nnormal_dfs = load_filtered_dfs(data_path, \"normal\")\nimbalance_dfs = load_filtered_dfs(data_path, \"imbalance-6g\")\n\n# Apply augmentation (downsampling + rolling) to each DF, then concat\nnormal_df = pd.concat([augment_features(df) for df in normal_dfs], ignore_index=True)\nimbalance_df = pd.concat([augment_features(df) for df in imbalance_dfs], ignore_index=True)\n\n# Label the data\nnormal_df[\"label\"] = 0\nimbalance_df[\"label\"] = 1\n\nall_data = pd.concat([normal_df, imbalance_df], ignore_index=True)\n\n# Show correlation matrix\nsave_correlation_matrix(all_data)\n\n# t-SNE visualization\nplot_tsne(all_data, label_column='label', output_file=\"../figures/tsne_visualization.png\")\n\n# Time-series split (train/val/test)\ntrain_data, val_data, test_data = time_series_split(all_data)\n\n# Normalize features\nscaler = StandardScaler()\ntrain_data.iloc[:, :-1] = scaler.fit_transform(train_data.iloc[:, :-1])\nval_data.iloc[:, :-1] = scaler.transform(val_data.iloc[:, :-1])\ntest_data.iloc[:, :-1] = scaler.transform(test_data.iloc[:, :-1])\n\n# Datasets & Loaders\ntrain_dataset = MachineryDataset(train_data)\nval_dataset = MachineryDataset(val_data)\ntest_dataset = MachineryDataset(test_data)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Initialize and train MLP\nmodel = TimeSeriesMLP(\n    input_dim=train_dataset.features.shape[1],\n    hidden_dim=3,\n    n_layers=2\n)\nhistory = train_model(model, train_loader, val_loader)  # default epochs=50\n\n# Evaluate\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\ntest_metrics = evaluate_model(model, test_loader)\nplot_evaluation_results(test_metrics, output_file=\"../figures/evaluation_plot.png\")\n\n\nAfter training, we apply the model to the test set. For binary classification, we typically measure:\n\nAccuracy: Ratio of correct predictions over total.\n\nF1-score: Harmonic mean of precision & recall.\n\nPrecision: Among predicted positives, how many are truly positive?\n\nRecall: Among all actual positives, how many did we predict correctly?\n\n\n\n\nEvaluation of MLP\n\n\nExcellent performance, which suggests that even a relatively straightforward MLP can separate normal vs. imbalance classes well, thanks to feature engineering (downsampling + rolling mean).\n\n\n\n\n\n\nHyperparameter Optimization\n\nTest different hidden layer sizes, dropout probabilities, and learning rates.\n\nConsider GridSearch or Bayesian Optimization for an automated approach.\n\nInclude More Fault Conditions\n\nThe MAFAULDA dataset has multiple fault types (unbalance, misalignment, bearing faults). Extending beyond just normal vs. imbalance classification can add realism.\n\nSequence Models\n\nFor a deeper time-series approach, experiment with CNNs (1D Convolutions) or LSTM architectures. Those are better at capturing sequential dependencies without relying only on rolling averages.\n\nReal-Time Inference\n\nDeploy the trained model in a streaming or edge environment for real-time fault detection in industrial settings.\n\n\n\n\n\n\nWe’ve walked through the basic steps to a complete pipeline for classifying mechanical faults using time-series data. The key lessons include:\n\nEDA is indispensable for quickly assessing data quality and distributions.\n\nFeature Engineering (downsampling, rolling means) can convert raw time series into useful representations to train deep learning models that does not include temporal dependencies inherently.\nEven a basic MLP can achieve high accuracy if the features reflect the underlying process well.\n\nEvaluation metrics (Accuracy, F1, Precision, Recall) are critical to understand true performance.\n\nIf you’re looking to adapt this pipeline to your own fault classification tasks—whether it’s rotating machinery, bearings, or other mechanical equipment—these concepts should be straightforward to customize. Feel free to explore the MAFAULDA dataset for your own experiments or extend it with advanced deep learning architectures.\nThanks for reading, and happy fault detecting!"
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#exploratory-data-analysis-eda",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#exploratory-data-analysis-eda",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "",
    "text": "We are dealing with vibration and microphone signals recorded from a motor simulator under two conditions:\n1. Normal\n2. Imbalance (faulty)\nThere are other types of faulty data present in the MAFAULDA’s website, but we only consider normal and imbalance (6g) datasets. After donwloading the data, we have this folder structure:\ndata\n├── imbalance\n│   └── imbalance\n│       ├── 10g\n│       ├── 15g\n│       ├── 20g\n│       ├── 25g\n│       ├── 30g\n│       ├── 35g\n│       └── 6g\n└── normal\n    └── normal\nAt each labeled folder presented above, there are multiple recordings stored in CSV files. Each recording is about 5 seconds at a 50 kHz sampling rate, resulting in 250,000 samples per sensor. The dataset includes eight features:\n\ntachometer\nunderhang_axial\nunderhang_radiale\nunderhang_tangential\noverhang_axial\noverhang_radiale\noverhang_tangential\nmicrophone\n\nThe first step is to get a sense of data distributions, missing values, basic statistics, etc. In EDA.ipynb, we use ydata-profiling package for a quickly review of how data looklike:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ydata_profiling import ProfileReport\n\n# Loading a CSV file for normal condition\ncol_names = [\n    'tachometer', 'underhang_axial', 'underhang_radiale', 'underhang_tangential',\n    'overhang_axial', 'overhang_radiale', 'overhang_tangential', 'microphone'\n]\nnormal_df = pd.read_csv(\"path/to/some_normal.csv\", names=col_names)\n\n# Generate HTML report (ydata-profiling)\nprofile = ProfileReport(normal_df, title=\"Normal Data\")\nprofile.to_file(\"normal_data_report.html\")\nThe generated report normal_data_report.html reveals distributions and correlations. This helps us spot which features might be redundant or highly correlated. For instance, you might notice that microphone and underhang_radiale have high correlation, which could inform feature selection later. Also, we noticed how the features are normally distributed, which could be lead to a smoother convergence when training models using Gradient descent. Besides being normally distributed, standardizing features (zero mean, unit variance) remains an important aspect of preprocessing.\n\n\n\nTo visualize how signal data changes over time, we can create quick plots of 50,000 samples (1 second snippet) from each class:\ndef plot_timeseries(df, columns, n_samples=50000):\n    plt.figure(figsize=(12, len(columns)))\n    for i, col in enumerate(columns, 1):\n        plt.subplot(len(columns), 1, i)\n        plt.plot(df[col].values[:n_samples])\n        plt.title(f\"Time Series of {col}\")\n    plt.tight_layout()\n    plt.show()\n\nplot_timeseries(normal_df, columns=normal_df.columns)\n\n\n\nNormal machinery dataset"
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#feature-engineering",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#feature-engineering",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "",
    "text": "High-frequency time-series data can be noisy and large, so feature engineering becomes crucial. Two primary transformations were used in FeatureEngineering.ipynb and ultimately in the main pipeline:\n\nDownsampling: Consolidates raw data by averaging every b samples. In the repository, b=2500 was used, reducing 250,000 samples per file to just 100 samples.\n\nRolling Mean: Applies a moving average (with a given window size) to smooth abrupt fluctuations and incorporate temporal context into each feature.\n\nIn FeatureEngineering.ipynb, you’ll see:\ndef downSampler(data, b):\n    \"\"\"\n    Downsamples the given DataFrame by averaging every 'b' rows.\n    \"\"\"\n    return data.groupby(data.index // b).mean().reset_index(drop=True)\n\n\ndef rolling_mean_data(df, window_size=100, columns=None):\n    \"\"\"\n    Applies a rolling mean transformation to specified columns.\n    \"\"\"\n    if columns is None:\n        columns = df.select_dtypes(include=[np.number]).columns\n\n    df_copy = df.copy()\n    df_copy[columns] = df_copy[columns].rolling(window=window_size, min_periods=1).mean()\n    return df_copy\n\n# Usage:\nnormal_df = downSampler(normal_df, 2500)\nnormal_df = rolling_mean_data(normal_df, window_size=100)\nAfter applying both transformation, the behaviour of each feature across the time is presented below:\n\n\n\nNormal data after feature engineering transformations\n\n\n\n\nAfter feature transformations, we often test whether the classes (normal vs. imbalance) are more distinguishable, and we can visually check this using dimensionality reduction methods such as Principal component analysis for linear transformation or t-distributed Stochastic Neighbor Embedding for non-linear transformation. In this case, we apply t-SNE as it is capable of dealing with non-linear transformation.\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n\ndef plot_tsne(df, label_column='label'):\n    features = df.select_dtypes(include=[np.number]).drop(columns=[label_column], errors='ignore')\n    features_scaled = StandardScaler().fit_transform(features)\n   \n    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n    tsne_results = tsne.fit_transform(features_scaled)\n\n    # Plot\n    plt.figure(figsize=(8, 6))\n    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=df[label_column], cmap=\"viridis\", alpha=0.7)\n    plt.title(\"t-SNE Visualization\")\n    plt.show()\nThe following t-SNE plot clearly shows how both classes can be distinguished visually. This visualization gives us confidence that there is a non-linear transformation capable of producing a rule that will correctly classify the binary time-series dataset.\n\n\n\nt-SNE for visualizing distribution of binary classes"
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#building-training-a-multi-layer-perceptron-mlp-in-pytorch",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#building-training-a-multi-layer-perceptron-mlp-in-pytorch",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "",
    "text": "While more sophisticated sequence models (e.g., LSTM, 1D CNNs) are more relevant for time-series data, a Multi-Layer Perceptron is a basic architecture that does not intrinsically include temporal dependencies. Since we have applied feature engineering capable of introducing basic temporal effects to features, the MLP might be sufficient for correctly classify this dataset. In general, if sequential dependencies matter, alternatives like RNN, LSTM, 1-D CNN, or Transformers may perform better, whereas MLP is effective for feature-based classification when raw time series is converted into useful representations through feature engineering.\n\n\nIn PyTorch, we create a custom Dataset class to handle how features and labels are fed to the model:\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass MachineryDataset(Dataset):\n    def __init__(self, data, label_column='label'):\n        self.labels = data[label_column].values.astype(np.float32)\n        self.features = data.drop(columns=[label_column, 'time'], errors='ignore').values.astype(np.float32)\n       \n    def __len__(self):\n        return len(self.features)\n   \n    def __getitem__(self, idx):\n        x = self.features[idx]\n        y = self.labels[idx]\n        return x, y\n\n__getitem__: Returns a single sample (features, label).\n__len__: Provides the total length of the dataset.\n\nWe also build a DataLoader object that batches the data and shuffles it during training:\ntrain_dataset = MachineryDataset(all_data, label_column='label')\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n\n\nA simple feed-forward neural network can be built using fully connected layers (nn.Linear):\nimport torch.nn as nn\n\nclass TimeSeriesMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, n_layers, dropout_prob=0.3):\n        super(TimeSeriesMLP, self).__init__()\n        layers = []\n        layers.append(nn.Linear(input_dim, hidden_dim))\n        layers.append(nn.ReLU())\n        layers.append(nn.Dropout(dropout_prob))\n\n        for _ in range(n_layers - 1):\n            layers.append(nn.Linear(hidden_dim, hidden_dim))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(dropout_prob))\n\n        layers.append(nn.Linear(hidden_dim, 1))\n        layers.append(nn.Sigmoid())\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\nHidden Layers and Activation:\nThe model consists of n_layers hidden layers, each applying a linear transformation (nn.Linear), followed by ReLU activation (nn.ReLU()).\nReLU (Rectified Linear Unit) is used because it helps mitigate the vanishing gradient problem and accelerates training.\nDropout Regularization:\nA dropout layer (nn.Dropout(dropout_prob)) is applied after each hidden layer to reduce overfitting.\nDropout randomly disables a fraction of neurons during training, forcing the model to learn more robust features.\nOutput Layer and Activation:\nThe final layer maps the last hidden representation to a single output neuron using nn.Linear(hidden_dim, 1).\nSigmoid activation (nn.Sigmoid()) is applied to ensure the output is in the range [0, 1], making it suitable for binary classification.\n\n\n\n\nThe train_model function in src/main.py file trains the MLP using a binary classification approach, tracking both loss and accuracy.\nimport torch.optim as optim\nimport torch\n\ndef train_model(model, train_loader, val_loader, epochs=50, lr=0.0005):\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n   \n    history = {\n        'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []\n    }\n   \n    for epoch in range(epochs):\n        model.train()  # Enable training mode\n        epoch_train_loss = 0\n        correct_train = 0\n        total_train = 0\n       \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs).squeeze()  # Forward pass\n            loss = criterion(outputs, targets)  # Compute loss\n            loss.backward()  # Backpropagation\n            optimizer.step()  # Update weights\n           \n            epoch_train_loss += loss.item()\n            preds = (outputs &gt; 0.5).float()  # Convert probabilities to binary predictions\n            correct_train += (preds == targets).sum().item()\n            total_train += targets.size(0)\n       \n        train_accuracy = correct_train / total_train  # Compute training accuracy\n       \n        model.eval()  # Enable evaluation mode\n        epoch_val_loss = 0\n        correct_val = 0\n        total_val = 0\n       \n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                outputs = model(inputs).squeeze()\n                val_loss = criterion(outputs, targets)\n                epoch_val_loss += val_loss.item()\n               \n                preds = (outputs &gt; 0.5).float()\n                correct_val += (preds == targets).sum().item()\n                total_val += targets.size(0)\n       \n        val_accuracy = correct_val / total_val  # Compute validation accuracy\n       \n        # Store metrics for analysis\n        history['train_loss'].append(epoch_train_loss / len(train_loader))\n        history['val_loss'].append(epoch_val_loss / len(val_loader))\n        history['train_acc'].append(train_accuracy)\n        history['val_acc'].append(val_accuracy)\n       \n        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {epoch_val_loss / len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n   \n    return history\n\nBinary Cross-Entropy Loss (nn.BCELoss()):\n\nSuitable for binary classification, where the target labels are 0 or 1.\n\nAdam Optimizer (optim.Adam()):\n\nAdaptive learning rate for better convergence.\n\nAccuracy Tracking:\n\nUses thresholding (outputs &gt; 0.5) to determine class predictions.\nCompares predictions to true labels (targets) to compute accuracy.\n\nTraining & Validation Loss History:\n\nLogs loss and accuracy at each epoch for performance monitoring."
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#putting-it-all-together",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#putting-it-all-together",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "",
    "text": "Finally, in src/main.py we orchestrate the entire workflow: 1. Data Ingestion & Labeling\n2. Feature Engineering (Downsampling, Rolling Mean, StandardScaler)\n3. Splitting (Time-series split into train/val/test sets)\n4. MLP Training\n5. Evaluation: Accuracy, F1, Precision, Recall, AUC-ROC\nBelow is a condensed snippet showing the pipeline’s main logic:\n# Load normal and imbalance data\nnormal_dfs = load_filtered_dfs(data_path, \"normal\")\nimbalance_dfs = load_filtered_dfs(data_path, \"imbalance-6g\")\n\n# Apply augmentation (downsampling + rolling) to each DF, then concat\nnormal_df = pd.concat([augment_features(df) for df in normal_dfs], ignore_index=True)\nimbalance_df = pd.concat([augment_features(df) for df in imbalance_dfs], ignore_index=True)\n\n# Label the data\nnormal_df[\"label\"] = 0\nimbalance_df[\"label\"] = 1\n\nall_data = pd.concat([normal_df, imbalance_df], ignore_index=True)\n\n# Show correlation matrix\nsave_correlation_matrix(all_data)\n\n# t-SNE visualization\nplot_tsne(all_data, label_column='label', output_file=\"../figures/tsne_visualization.png\")\n\n# Time-series split (train/val/test)\ntrain_data, val_data, test_data = time_series_split(all_data)\n\n# Normalize features\nscaler = StandardScaler()\ntrain_data.iloc[:, :-1] = scaler.fit_transform(train_data.iloc[:, :-1])\nval_data.iloc[:, :-1] = scaler.transform(val_data.iloc[:, :-1])\ntest_data.iloc[:, :-1] = scaler.transform(test_data.iloc[:, :-1])\n\n# Datasets & Loaders\ntrain_dataset = MachineryDataset(train_data)\nval_dataset = MachineryDataset(val_data)\ntest_dataset = MachineryDataset(test_data)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Initialize and train MLP\nmodel = TimeSeriesMLP(\n    input_dim=train_dataset.features.shape[1],\n    hidden_dim=3,\n    n_layers=2\n)\nhistory = train_model(model, train_loader, val_loader)  # default epochs=50\n\n# Evaluate\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\ntest_metrics = evaluate_model(model, test_loader)\nplot_evaluation_results(test_metrics, output_file=\"../figures/evaluation_plot.png\")\n\n\nAfter training, we apply the model to the test set. For binary classification, we typically measure:\n\nAccuracy: Ratio of correct predictions over total.\n\nF1-score: Harmonic mean of precision & recall.\n\nPrecision: Among predicted positives, how many are truly positive?\n\nRecall: Among all actual positives, how many did we predict correctly?\n\n\n\n\nEvaluation of MLP\n\n\nExcellent performance, which suggests that even a relatively straightforward MLP can separate normal vs. imbalance classes well, thanks to feature engineering (downsampling + rolling mean)."
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#next-steps-enhancements",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#next-steps-enhancements",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "",
    "text": "Hyperparameter Optimization\n\nTest different hidden layer sizes, dropout probabilities, and learning rates.\n\nConsider GridSearch or Bayesian Optimization for an automated approach.\n\nInclude More Fault Conditions\n\nThe MAFAULDA dataset has multiple fault types (unbalance, misalignment, bearing faults). Extending beyond just normal vs. imbalance classification can add realism.\n\nSequence Models\n\nFor a deeper time-series approach, experiment with CNNs (1D Convolutions) or LSTM architectures. Those are better at capturing sequential dependencies without relying only on rolling averages.\n\nReal-Time Inference\n\nDeploy the trained model in a streaming or edge environment for real-time fault detection in industrial settings."
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#conclusion",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#conclusion",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "",
    "text": "We’ve walked through the basic steps to a complete pipeline for classifying mechanical faults using time-series data. The key lessons include:\n\nEDA is indispensable for quickly assessing data quality and distributions.\n\nFeature Engineering (downsampling, rolling means) can convert raw time series into useful representations to train deep learning models that does not include temporal dependencies inherently.\nEven a basic MLP can achieve high accuracy if the features reflect the underlying process well.\n\nEvaluation metrics (Accuracy, F1, Precision, Recall) are critical to understand true performance.\n\nIf you’re looking to adapt this pipeline to your own fault classification tasks—whether it’s rotating machinery, bearings, or other mechanical equipment—these concepts should be straightforward to customize. Feel free to explore the MAFAULDA dataset for your own experiments or extend it with advanced deep learning architectures.\nThanks for reading, and happy fault detecting!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Data Scientist with a strong background in machine learning, optimization, and MLOps, focused on developing scalable AI solutions and deploying models into production. I hold a Bachelor’s in Electronic Engineering and a Master’s in Information Systems, with international academic experience across Japan, the United Kingdom, and Canada. This diverse background has equipped me with strong analytical skills, the ability to quickly understand and implement state-of-the-art research, and a scientific approach to solving complex industry challenges.\nIn my industry experience, I have worked extensively with machine learning and deep learning frameworks to train and fine-tune models for predictive modeling, anomaly detection, and computer vision. My expertise in MLOps enables me to design and implement end-to-end pipelines that ensure reproducibility and scalability. Leveraging tools like MLflow, Docker, and cloud platforms like AWS and DigitalOcean, I have successfully deployed machine learning models and integrated them into production via API services.\nWith a strong statistical analysis and optimization foundation, I am particularly interested in Generative AI and LLMs, actively exploring their potential in real-world applications. I am eager to contribute to cutting-edge AI solutions and look forward to collaborating with professionals and organizations driving AI innovation in production environments."
  },
  {
    "objectID": "about.html#technical-expertise",
    "href": "about.html#technical-expertise",
    "title": "About",
    "section": "Technical Expertise",
    "text": "Technical Expertise\n\nOperating Systems:\n Linux (Fedora, Ubuntu),  Windows.\n\n\n\nProgramming Languages:\n Python,  C,  C++,  Bash (Unix Shell).\n\n\n\nMachine Learning Frameworks:\n PyTorch,  scikit-learn,  XGBoost, FastAI.\n\n\n\nMLOps & Deployment Tools:\n Docker,  MLflow,  Hydra,  Git,  CI/CD (GitLab, GitHub Actions),  FastAPI.\n\n\n\nData Engineering & Pipelines:\n NumPy,  Pandas, Dask,  PostgreSQL, Redshift, InfluxDB.\n\n\n\nCloud Platforms:\n AWS,  DigitalOcean.\n\n\n\nOptimization:\nBayesian Optimization, Multi-objective Evolutionary Algorithms."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "In the dynamic world of short-term property rentals, accurately predicting rental prices is crucial for maximizing occupancy and revenue. To address this challenge, I developed a comprehensive, reproducible Machine Learning (ML) pipeline tailored for short-term rental price prediction in New York City (NYC) based on Airbnb data. We walk through data collection, cleaning, validation, splitting, training a random forest model, testing the model, and logging every artifact and result in a reproducible manner. This blog post focus on the usage of MLflow, Weights & Biases (W&B), and Hydra for orchestration, tracking, and hyperparameter optimization, as these are taught in the Udacity MLOps Nanodegree program.\n\n\nShort-term rental platforms like Airbnb collect vast amounts of data from hosts and guests. A host often asks: What price should I set for my listing to optimize occupancy and revenue? This project aims to solve that by building an end-to-end ML pipeline to predict a short-term rental’s price given various features such as location, room type, and number of reviews.\n\nFocus: Clean, reproducible, and easily re-runnable pipeline.\nTools: MLflow for orchestration, Hydra for configuration, Weights & Biases to log artifacts and metrics, and scikit-learn for model training.\nAdditional: ydata-profiling for EDA, custom data checks to prevent “data drift,” and a templated approach to add new steps using a cookiecutter MLflow step.\n\nThe entire pipeline is tracked under a W&B project to enable experiment tracking and artifact storage. Here is the link for the completed project in W&B.\n\n\n\nThe project is organized as follows:\n.\n├── components/                  # Reusable pipeline components\n│   ├── get_data/                # Data ingestion module\n│   ├── test_regression_model/   # Model testing module\n│   └── train_val_test_split/    # Data splitting module\n├── src/                         # Source code for main pipeline steps\n│   ├── basic_cleaning/          # Data cleaning scripts\n│   ├── data_check/              # Data validation scripts\n│   ├── eda/                     # Exploratory Data Analysis scripts\n│   └── train_random_forest/     # Model training scripts\n├── images/                      # Visual assets for documentation\n├── .github/workflows/           # GitHub Actions for CI/CD\n├── cookie-mlflow-step/          # Template for creating new pipeline steps\n├── conda.yml                    # Conda environment configuration\n├── config.yaml                  # Pipeline configuration settings\n├── environment.yml              # Alternative environment configuration\n├── main.py                      # Main script to run the pipeline\n├── MLproject                    # MLflow project specification\n└── README.md                    # Project documentation\nKey folders:\n\ncomponents/: Reusable MLflow components for tasks like data downloading, data splitting, and model testing.\n\nsrc/: Specific pipeline steps, including EDA (eda), data cleaning (basic_cleaning), data checks (data_check), and training a random forest (train_random_forest).\n\ncookie-mlflow-step/: A cookiecutter template that quickly scaffolds new MLflow pipeline steps.\n\nmain.py: The orchestrator that references each step through Hydra configuration.\n\nMLproject: Defines how MLflow runs the pipeline, specifying entry points and environment details.\n\n\n\n\n\n\nThis module handles the retrieval of raw data, ensuring it’s stored in a structured format suitable for downstream processing. It interfaces with data sources, downloads datasets, and logs them as artifacts for version control.\n\n\n\nData cleaning involves:\n\nRemoving duplicates and irrelevant entries.\nHandling missing values through imputation or removal.\nCorrecting data types and formatting issues.\nAddressing outliers to prevent skewed model training.\n\n\n\n\nEDA provides insights into the dataset through:\n\nStatistical summaries of features.\nVisualizations to identify patterns and correlations.\nDetection of anomalies or unexpected distributions.\n\n\n\n\nBefore model training, data validation checks are performed to ensure:\n\nConsistency in data formats and ranges.\nIntegrity constraints are maintained.\nAlignment with expected distributions to prevent data drift.\n\n\n\n\nThe dataset is partitioned into:\n\nTraining Set: For model learning.\nValidation Set: For hyperparameter tuning and model selection.\nTest Set: For final evaluation of model performance.\n\n\n\n\nUtilizing the Random Forest algorithm, this step involves:\n\nTraining the model on the prepared dataset.\nLogging training parameters and metrics.\nSaving the trained model artifact for evaluation and deployment.\n\n\n\n\nThe trained model undergoes rigorous evaluation to assess:\n\nPredictive accuracy on unseen data.\nGeneralization capabilities.\nPotential overfitting or underfitting issues.\n\n\n\n\n\n\nWe rely on two environment files:\n\nenvironment.yml: Sets up the main environment (nyc_airbnb_dev) with Python 3.10, Hydra, Jupyter, and crucial Python packages (MLflow, W&B, ydata-profiling, etc.).\n\nconda.yml (in various subfolders): Each step can be run in a mini environment with the dependencies it needs.\n\nTo install:\nconda env create -f environment.yml\nconda activate nyc_airbnb_dev\nwandb login [your_API_key]\nAfter this, you can run:\nmlflow run . \nMLflow will pick up the MLproject file in the root directory and execute main.py.\n\n\n\n\n\n\nThe code for downloading data is stored in components/get_data. We keep a couple of sample CSVs in data/ that stand in for a real-world dataset. This step simply logs an artifact to W&B.\nCode Snippet from components/get_data/run.py\ndef go(args):\n    run = wandb.init(job_type=\"download_file\")\n    run.config.update(args)\n\n    log_artifact(\n        args.artifact_name,\n        args.artifact_type,\n        args.artifact_description,\n        os.path.join(\"data\", args.sample),\n        run,\n    )\nTo run just the download step:\nmlflow run . -P steps=download\n\n\n\nWe have an EDA folder (src/eda). It contains a EDA.ipynb notebook, which uses ydata-profiling for generating a quick profile report of the dataset. The snippet below shows how we generate an HTML report and log it to W&B.\nCode Snippet from src/eda/EDA.ipynb\nimport ydata_profiling\nprofile = ydata_profiling.ProfileReport(df)\nprofile.to_file(\"profile-report.html\")\nartifact = wandb.Artifact(\n    name=\"profile-report.html\", \n    type=\"analysis\", \n    description=\"Report from ydata-profiling\"\n)\nartifact.add_file(\"profile-report.html\")\nrun.log_artifact(artifact)\nWe also create various plots (like price distribution) and attempt to remove obvious outliers (price &lt;10 or &gt;350) to get a more reasonable dataset.\nKey Observations:\n- Some columns like last_review and reviews_per_month can have many missing values.\n- The distribution of price is highly skewed.\n- We remove out-of-bound lat/long values that are not within the known NYC boundaries.\n\n\n\nA dedicated step in src/basic_cleaning/ cleans the data after EDA reveals certain constraints.\nWe used Cookiecutter to generate the skeleton for this step and then filled in the logic. The run.py file:\nmlflow run . -P steps=basic_cleaning\nCore snippet from src/basic_cleaning/run.py:\ndf = df.drop_duplicates().reset_index(drop=True)\nidx = df['price'].between(args.min_price, args.max_price)\ndf = df[idx].copy()\nidx = df['longitude'].between(-74.25, -73.50) & df['latitude'].between(40.5, 41.2)\ndf = df[idx].copy()\n\n# Log the cleaned CSV as W&B artifact\nartifact = wandb.Artifact(\n    name=args.output_artifact,\n    type=args.output_type,\n    description=args.output_description,\n)\nartifact.add_file(fp.name)\nrun.log_artifact(artifact)\nIt:\n1. Drops duplicates.\n2. Filters out out-of-range prices.\n3. Ensures lat/long within valid NYC boundary.\n4. Logs the cleaned dataset to W&B.\n\n\n\nWe follow the concept of Data Testing to guard against “data pipeline rot.” The step is in src/data_check/. It uses pytest tests to verify that the cleaned data:\n\nHas valid column names.\nFalls within expected lat/long boundaries.\nContains only known neighborhoods.\nDistributes similarly to a reference dataset (via KL divergence).\nRespects a minimum and maximum price range.\n\nA snippet of the test suite from src/data_check/test_data.py:\ndef test_similar_neigh_distrib(data: pd.DataFrame, ref_data: pd.DataFrame, kl_threshold: float):\n    dist1 = data['neighbourhood_group'].value_counts().sort_index()\n    dist2 = ref_data['neighbourhood_group'].value_counts().sort_index()\n    assert scipy.stats.entropy(dist1, dist2, base=2) &lt; kl_threshold\nTo run:\nmlflow run . -P steps=data_check\nAny mismatches or anomalies raise an exception that stops the pipeline, keeping you from training on “bad” data.\n\n\n\nWe then split our dataset into training, validation, and test sets (the last set is strictly for final model testing). The relevant code is in components/train_val_test_split/.\nCode Snippet from components/train_val_test_split/run.py\ntrainval, test = train_test_split(\n    df,\n    test_size=args.test_size,\n    random_state=args.random_seed,\n    stratify=df[args.stratify_by] if args.stratify_by != 'none' else None,\n)\nBoth the training/validation split (“trainval_data.csv”) and the test split (“test_data.csv”) are then logged to W&B.\n\n\n\nWith a clean train-validation dataset, we build a random forest pipeline in src/train_random_forest/run.py. This step is heavily reliant on Hydra for configuration. We define parameters like max_depth, n_estimators, etc., in config.yaml.\nHere’s a highlight from run.py:\ndef get_inference_pipeline(rf_config, max_tfidf_features):\n    ordinal_categorical = [\"room_type\"]\n    non_ordinal_categorical = [\"neighbourhood_group\"]\n    \n    ordinal_categorical_preproc = OrdinalEncoder()\n    non_ordinal_categorical_preproc = Pipeline([\n        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"encode\", OneHotEncoder())\n    ])\n    ...\n    random_forest = RandomForestRegressor(**rf_config)\n    sk_pipe = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"random_forest\", random_forest),\n    ])\n    return sk_pipe, processed_features\nIn this pipeline, we handle:\n- Categorical columns (OrdinalEncoder or OneHotEncoder).\n- Numerical columns (imputation for missing values).\n- NLP on the name field using a TF-IDF vectorizer.\nWe train and evaluate on a validation set, logging all metrics (MAE, R^2) to W&B. Finally, the pipeline (preprocessing + model) is saved to MLflow format and uploaded to W&B as an artifact:\nmlflow.sklearn.save_model(\n    sk_pipe,\n    export_path,\n    serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE,\n    signature=sig,\n    input_example=X_val[processed_features].iloc[:2]\n)\nBy storing the entire pipeline, we can apply transformations consistently at inference time.\nTo run the model training step:\nmlflow run . -P steps=train_random_forest\n\n\n\nLastly, we evaluate our finalized model against the test set. The code is in components/test_regression_model/run.py. It:\n\nDownloads the prod model artifact from W&B.\n\nLoads the test dataset.\n\nGenerates predictions and calculates R^2 and MAE.\n\nLogs test performance to W&B.\n\ndef go(args):\n    sk_pipe = mlflow.sklearn.load_model(model_local_path)\n    y_pred = sk_pipe.predict(X_test)\n    r_squared = sk_pipe.score(X_test, y_test)\n    mae = mean_absolute_error(y_test, y_pred)\n\n    run.summary['r2'] = r_squared\n    run.summary['mae'] = mae\nYou only run this step after a model has been tagged for production, ensuring it has proven to be stable in dev/validation.\n\n\n\n\n\nThe main.py script orchestrates the entire flow. It reads config.yaml via Hydra and decides which steps to run based on a comma-separated list. A minimal snippet:\n_steps = [\n    \"download\",\n    \"basic_cleaning\",\n    \"data_check\",\n    \"data_split\",\n    \"train_random_forest\",\n    # \"test_regression_model\" is triggered separately\n]\n\n@hydra.main(config_name='config')\ndef go(config: DictConfig):\n\n    steps_par = config['main']['steps']\n    active_steps = steps_par.split(\",\") if steps_par != \"all\" else _steps\n\n    if \"download\" in active_steps:\n        mlflow.run(\n            f\"{config['main']['components_repository']}/get_data\",\n            ...\n        )\n\n    if \"basic_cleaning\" in active_steps:\n        mlflow.run(\n            os.path.join(hydra.utils.get_original_cwd(), \"src\", \"basic_cleaning\"),\n            ...\n        )\n\n    ...\nTo execute the entire pipeline:\nmlflow run . \nOr pick and choose steps:\nmlflow run . -P steps=download,basic_cleaning,data_check\n\n\n\n\nOne of the repository’s highlights is the cookie-mlflow-step folder, which speeds up adding new steps to the pipeline:\ncookiecutter cookie-mlflow-step -o src\nIt scaffolds a new directory structure containing an MLproject, conda.yml, and a run.py pre-populated with arguments. This is helpful for consistent, standardized pipeline steps where each step is an MLflow project.\n\n\n\n\n\nCI/CD: The .github/workflows/manual.yml shows how a manual GH Action can create Jira tickets for new Pull Requests.\n\nRelease: Tagging your repo with a version number (e.g., 1.0.0) allows you to run the pipeline from a specific commit. For instance:\nmlflow run https://github.com/yurimarca/build-ml-pipeline-for-short-term-rental-prices.git \\\n  -v 1.0.0 \\\n  -P hydra_options=\"etl.sample='sample2.csv'\"\nArtifact Logging: W&B captures every CSV and model artifact, so future steps or collaborators can trace lineage and retrieve them easily.\n\n\n\n\n\nThis repository combines Hydra for flexible configuration, MLflow for pipeline orchestration, and Weights & Biases for experiment tracking to create a fully reproducible short-term rental price prediction pipeline in NYC.\nKey Takeaways:\n\nData integrity: By integrating data validation tests, the pipeline can fail early if data is incorrect.\n\nReproducible training: Using Hydra, MLflow, and environment files ensures consistent environments and parameter definitions.\n\nFull pipeline tracking: From EDA to final test, each artifact is logged to W&B, making it easy to revert or compare different runs.\n\nExtensibility: The cookiecutter approach helps you quickly add new pipeline steps or replicate the same pipeline structure in other projects.\n\nFeel free to explore the code base, and don’t hesitate to experiment by customizing steps or hyperparameters. By following this pipeline, you can keep your machine learning workflow tidy, versioned, and production-ready.\nFor a detailed walkthrough and access to the codebase, visit the GitHub repository.\nNote: This project was developed as part of the Udacity MLOps Nanodegree program."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#project-overview",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#project-overview",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "Short-term rental platforms like Airbnb collect vast amounts of data from hosts and guests. A host often asks: What price should I set for my listing to optimize occupancy and revenue? This project aims to solve that by building an end-to-end ML pipeline to predict a short-term rental’s price given various features such as location, room type, and number of reviews.\n\nFocus: Clean, reproducible, and easily re-runnable pipeline.\nTools: MLflow for orchestration, Hydra for configuration, Weights & Biases to log artifacts and metrics, and scikit-learn for model training.\nAdditional: ydata-profiling for EDA, custom data checks to prevent “data drift,” and a templated approach to add new steps using a cookiecutter MLflow step.\n\nThe entire pipeline is tracked under a W&B project to enable experiment tracking and artifact storage. Here is the link for the completed project in W&B."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#repository-structure",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#repository-structure",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "The project is organized as follows:\n.\n├── components/                  # Reusable pipeline components\n│   ├── get_data/                # Data ingestion module\n│   ├── test_regression_model/   # Model testing module\n│   └── train_val_test_split/    # Data splitting module\n├── src/                         # Source code for main pipeline steps\n│   ├── basic_cleaning/          # Data cleaning scripts\n│   ├── data_check/              # Data validation scripts\n│   ├── eda/                     # Exploratory Data Analysis scripts\n│   └── train_random_forest/     # Model training scripts\n├── images/                      # Visual assets for documentation\n├── .github/workflows/           # GitHub Actions for CI/CD\n├── cookie-mlflow-step/          # Template for creating new pipeline steps\n├── conda.yml                    # Conda environment configuration\n├── config.yaml                  # Pipeline configuration settings\n├── environment.yml              # Alternative environment configuration\n├── main.py                      # Main script to run the pipeline\n├── MLproject                    # MLflow project specification\n└── README.md                    # Project documentation\nKey folders:\n\ncomponents/: Reusable MLflow components for tasks like data downloading, data splitting, and model testing.\n\nsrc/: Specific pipeline steps, including EDA (eda), data cleaning (basic_cleaning), data checks (data_check), and training a random forest (train_random_forest).\n\ncookie-mlflow-step/: A cookiecutter template that quickly scaffolds new MLflow pipeline steps.\n\nmain.py: The orchestrator that references each step through Hydra configuration.\n\nMLproject: Defines how MLflow runs the pipeline, specifying entry points and environment details."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#pipeline-components",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#pipeline-components",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "This module handles the retrieval of raw data, ensuring it’s stored in a structured format suitable for downstream processing. It interfaces with data sources, downloads datasets, and logs them as artifacts for version control.\n\n\n\nData cleaning involves:\n\nRemoving duplicates and irrelevant entries.\nHandling missing values through imputation or removal.\nCorrecting data types and formatting issues.\nAddressing outliers to prevent skewed model training.\n\n\n\n\nEDA provides insights into the dataset through:\n\nStatistical summaries of features.\nVisualizations to identify patterns and correlations.\nDetection of anomalies or unexpected distributions.\n\n\n\n\nBefore model training, data validation checks are performed to ensure:\n\nConsistency in data formats and ranges.\nIntegrity constraints are maintained.\nAlignment with expected distributions to prevent data drift.\n\n\n\n\nThe dataset is partitioned into:\n\nTraining Set: For model learning.\nValidation Set: For hyperparameter tuning and model selection.\nTest Set: For final evaluation of model performance.\n\n\n\n\nUtilizing the Random Forest algorithm, this step involves:\n\nTraining the model on the prepared dataset.\nLogging training parameters and metrics.\nSaving the trained model artifact for evaluation and deployment.\n\n\n\n\nThe trained model undergoes rigorous evaluation to assess:\n\nPredictive accuracy on unseen data.\nGeneralization capabilities.\nPotential overfitting or underfitting issues."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#setting-up-the-environment",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#setting-up-the-environment",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "We rely on two environment files:\n\nenvironment.yml: Sets up the main environment (nyc_airbnb_dev) with Python 3.10, Hydra, Jupyter, and crucial Python packages (MLflow, W&B, ydata-profiling, etc.).\n\nconda.yml (in various subfolders): Each step can be run in a mini environment with the dependencies it needs.\n\nTo install:\nconda env create -f environment.yml\nconda activate nyc_airbnb_dev\nwandb login [your_API_key]\nAfter this, you can run:\nmlflow run . \nMLflow will pick up the MLproject file in the root directory and execute main.py."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#pipeline-steps",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#pipeline-steps",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "The code for downloading data is stored in components/get_data. We keep a couple of sample CSVs in data/ that stand in for a real-world dataset. This step simply logs an artifact to W&B.\nCode Snippet from components/get_data/run.py\ndef go(args):\n    run = wandb.init(job_type=\"download_file\")\n    run.config.update(args)\n\n    log_artifact(\n        args.artifact_name,\n        args.artifact_type,\n        args.artifact_description,\n        os.path.join(\"data\", args.sample),\n        run,\n    )\nTo run just the download step:\nmlflow run . -P steps=download\n\n\n\nWe have an EDA folder (src/eda). It contains a EDA.ipynb notebook, which uses ydata-profiling for generating a quick profile report of the dataset. The snippet below shows how we generate an HTML report and log it to W&B.\nCode Snippet from src/eda/EDA.ipynb\nimport ydata_profiling\nprofile = ydata_profiling.ProfileReport(df)\nprofile.to_file(\"profile-report.html\")\nartifact = wandb.Artifact(\n    name=\"profile-report.html\", \n    type=\"analysis\", \n    description=\"Report from ydata-profiling\"\n)\nartifact.add_file(\"profile-report.html\")\nrun.log_artifact(artifact)\nWe also create various plots (like price distribution) and attempt to remove obvious outliers (price &lt;10 or &gt;350) to get a more reasonable dataset.\nKey Observations:\n- Some columns like last_review and reviews_per_month can have many missing values.\n- The distribution of price is highly skewed.\n- We remove out-of-bound lat/long values that are not within the known NYC boundaries.\n\n\n\nA dedicated step in src/basic_cleaning/ cleans the data after EDA reveals certain constraints.\nWe used Cookiecutter to generate the skeleton for this step and then filled in the logic. The run.py file:\nmlflow run . -P steps=basic_cleaning\nCore snippet from src/basic_cleaning/run.py:\ndf = df.drop_duplicates().reset_index(drop=True)\nidx = df['price'].between(args.min_price, args.max_price)\ndf = df[idx].copy()\nidx = df['longitude'].between(-74.25, -73.50) & df['latitude'].between(40.5, 41.2)\ndf = df[idx].copy()\n\n# Log the cleaned CSV as W&B artifact\nartifact = wandb.Artifact(\n    name=args.output_artifact,\n    type=args.output_type,\n    description=args.output_description,\n)\nartifact.add_file(fp.name)\nrun.log_artifact(artifact)\nIt:\n1. Drops duplicates.\n2. Filters out out-of-range prices.\n3. Ensures lat/long within valid NYC boundary.\n4. Logs the cleaned dataset to W&B.\n\n\n\nWe follow the concept of Data Testing to guard against “data pipeline rot.” The step is in src/data_check/. It uses pytest tests to verify that the cleaned data:\n\nHas valid column names.\nFalls within expected lat/long boundaries.\nContains only known neighborhoods.\nDistributes similarly to a reference dataset (via KL divergence).\nRespects a minimum and maximum price range.\n\nA snippet of the test suite from src/data_check/test_data.py:\ndef test_similar_neigh_distrib(data: pd.DataFrame, ref_data: pd.DataFrame, kl_threshold: float):\n    dist1 = data['neighbourhood_group'].value_counts().sort_index()\n    dist2 = ref_data['neighbourhood_group'].value_counts().sort_index()\n    assert scipy.stats.entropy(dist1, dist2, base=2) &lt; kl_threshold\nTo run:\nmlflow run . -P steps=data_check\nAny mismatches or anomalies raise an exception that stops the pipeline, keeping you from training on “bad” data.\n\n\n\nWe then split our dataset into training, validation, and test sets (the last set is strictly for final model testing). The relevant code is in components/train_val_test_split/.\nCode Snippet from components/train_val_test_split/run.py\ntrainval, test = train_test_split(\n    df,\n    test_size=args.test_size,\n    random_state=args.random_seed,\n    stratify=df[args.stratify_by] if args.stratify_by != 'none' else None,\n)\nBoth the training/validation split (“trainval_data.csv”) and the test split (“test_data.csv”) are then logged to W&B.\n\n\n\nWith a clean train-validation dataset, we build a random forest pipeline in src/train_random_forest/run.py. This step is heavily reliant on Hydra for configuration. We define parameters like max_depth, n_estimators, etc., in config.yaml.\nHere’s a highlight from run.py:\ndef get_inference_pipeline(rf_config, max_tfidf_features):\n    ordinal_categorical = [\"room_type\"]\n    non_ordinal_categorical = [\"neighbourhood_group\"]\n    \n    ordinal_categorical_preproc = OrdinalEncoder()\n    non_ordinal_categorical_preproc = Pipeline([\n        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"encode\", OneHotEncoder())\n    ])\n    ...\n    random_forest = RandomForestRegressor(**rf_config)\n    sk_pipe = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"random_forest\", random_forest),\n    ])\n    return sk_pipe, processed_features\nIn this pipeline, we handle:\n- Categorical columns (OrdinalEncoder or OneHotEncoder).\n- Numerical columns (imputation for missing values).\n- NLP on the name field using a TF-IDF vectorizer.\nWe train and evaluate on a validation set, logging all metrics (MAE, R^2) to W&B. Finally, the pipeline (preprocessing + model) is saved to MLflow format and uploaded to W&B as an artifact:\nmlflow.sklearn.save_model(\n    sk_pipe,\n    export_path,\n    serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE,\n    signature=sig,\n    input_example=X_val[processed_features].iloc[:2]\n)\nBy storing the entire pipeline, we can apply transformations consistently at inference time.\nTo run the model training step:\nmlflow run . -P steps=train_random_forest\n\n\n\nLastly, we evaluate our finalized model against the test set. The code is in components/test_regression_model/run.py. It:\n\nDownloads the prod model artifact from W&B.\n\nLoads the test dataset.\n\nGenerates predictions and calculates R^2 and MAE.\n\nLogs test performance to W&B.\n\ndef go(args):\n    sk_pipe = mlflow.sklearn.load_model(model_local_path)\n    y_pred = sk_pipe.predict(X_test)\n    r_squared = sk_pipe.score(X_test, y_test)\n    mae = mean_absolute_error(y_test, y_pred)\n\n    run.summary['r2'] = r_squared\n    run.summary['mae'] = mae\nYou only run this step after a model has been tagged for production, ensuring it has proven to be stable in dev/validation."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#putting-it-all-together-main.py",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#putting-it-all-together-main.py",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "The main.py script orchestrates the entire flow. It reads config.yaml via Hydra and decides which steps to run based on a comma-separated list. A minimal snippet:\n_steps = [\n    \"download\",\n    \"basic_cleaning\",\n    \"data_check\",\n    \"data_split\",\n    \"train_random_forest\",\n    # \"test_regression_model\" is triggered separately\n]\n\n@hydra.main(config_name='config')\ndef go(config: DictConfig):\n\n    steps_par = config['main']['steps']\n    active_steps = steps_par.split(\",\") if steps_par != \"all\" else _steps\n\n    if \"download\" in active_steps:\n        mlflow.run(\n            f\"{config['main']['components_repository']}/get_data\",\n            ...\n        )\n\n    if \"basic_cleaning\" in active_steps:\n        mlflow.run(\n            os.path.join(hydra.utils.get_original_cwd(), \"src\", \"basic_cleaning\"),\n            ...\n        )\n\n    ...\nTo execute the entire pipeline:\nmlflow run . \nOr pick and choose steps:\nmlflow run . -P steps=download,basic_cleaning,data_check"
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#using-cookiecutter-for-quick-pipeline-steps",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#using-cookiecutter-for-quick-pipeline-steps",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "One of the repository’s highlights is the cookie-mlflow-step folder, which speeds up adding new steps to the pipeline:\ncookiecutter cookie-mlflow-step -o src\nIt scaffolds a new directory structure containing an MLproject, conda.yml, and a run.py pre-populated with arguments. This is helpful for consistent, standardized pipeline steps where each step is an MLflow project."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#deployment-and-workflow",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#deployment-and-workflow",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "CI/CD: The .github/workflows/manual.yml shows how a manual GH Action can create Jira tickets for new Pull Requests.\n\nRelease: Tagging your repo with a version number (e.g., 1.0.0) allows you to run the pipeline from a specific commit. For instance:\nmlflow run https://github.com/yurimarca/build-ml-pipeline-for-short-term-rental-prices.git \\\n  -v 1.0.0 \\\n  -P hydra_options=\"etl.sample='sample2.csv'\"\nArtifact Logging: W&B captures every CSV and model artifact, so future steps or collaborators can trace lineage and retrieve them easily."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#conclusion",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#conclusion",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "This repository combines Hydra for flexible configuration, MLflow for pipeline orchestration, and Weights & Biases for experiment tracking to create a fully reproducible short-term rental price prediction pipeline in NYC.\nKey Takeaways:\n\nData integrity: By integrating data validation tests, the pipeline can fail early if data is incorrect.\n\nReproducible training: Using Hydra, MLflow, and environment files ensures consistent environments and parameter definitions.\n\nFull pipeline tracking: From EDA to final test, each artifact is logged to W&B, making it easy to revert or compare different runs.\n\nExtensibility: The cookiecutter approach helps you quickly add new pipeline steps or replicate the same pipeline structure in other projects.\n\nFeel free to explore the code base, and don’t hesitate to experiment by customizing steps or hyperparameters. By following this pipeline, you can keep your machine learning workflow tidy, versioned, and production-ready.\nFor a detailed walkthrough and access to the codebase, visit the GitHub repository.\nNote: This project was developed as part of the Udacity MLOps Nanodegree program."
  },
  {
    "objectID": "old_content/publication.html",
    "href": "old_content/publication.html",
    "title": "Yuri Marca’s Homepage",
    "section": "",
    "text": "Y. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. Approximating Pareto set topology by cubic interpolation on bi-objective problems. 10th International Conference on Evolutionary Multi-Criterion Optimization (EMO2019), Lecture Notes in Computer Science (LNCS), vol 11411, pp 386-398, East Lansing, Michigan, USA, 2019 DOI ★Best Student Paper Award★\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. NSGA-II with Spline Interpolation on Bi-objective Problems with Difficult Pareto Set Topology. JPNSEC 2018 Symposium on Evolutionary Computation, Fukuoka, 2018. ★Young Research Award★\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. MOEAs on Problems with Difficult Pareto Set Topologies. IEICE Shin-etsu Branch IEEE Session, Niigata University, 2018, p. 169. (presentation) ★Young Research Paper Award★\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. Pareto dominance-based MOEAs on problems with difficult pareto set topologies. In Proceedings of the Genetic and Evolutionary Computation Conference Companion (GECCO ’18). ACM, New York, NY, USA, 189-190. DOI.\nC. E. A. L. Rocha, Y. P. Marca, F. K. Schneider. Support Platform for Decision-Making in Research and Technological Development in Public Health. ESPACIOS (CARACAS), v. 39, p. 14-26, 2018. (link)"
  },
  {
    "objectID": "old_content/publication.html#publications",
    "href": "old_content/publication.html#publications",
    "title": "Yuri Marca’s Homepage",
    "section": "",
    "text": "Y. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. Approximating Pareto set topology by cubic interpolation on bi-objective problems. 10th International Conference on Evolutionary Multi-Criterion Optimization (EMO2019), Lecture Notes in Computer Science (LNCS), vol 11411, pp 386-398, East Lansing, Michigan, USA, 2019 DOI ★Best Student Paper Award★\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. NSGA-II with Spline Interpolation on Bi-objective Problems with Difficult Pareto Set Topology. JPNSEC 2018 Symposium on Evolutionary Computation, Fukuoka, 2018. ★Young Research Award★\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. MOEAs on Problems with Difficult Pareto Set Topologies. IEICE Shin-etsu Branch IEEE Session, Niigata University, 2018, p. 169. (presentation) ★Young Research Paper Award★\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. Pareto dominance-based MOEAs on problems with difficult pareto set topologies. In Proceedings of the Genetic and Evolutionary Computation Conference Companion (GECCO ’18). ACM, New York, NY, USA, 189-190. DOI.\nC. E. A. L. Rocha, Y. P. Marca, F. K. Schneider. Support Platform for Decision-Making in Research and Technological Development in Public Health. ESPACIOS (CARACAS), v. 39, p. 14-26, 2018. (link)"
  },
  {
    "objectID": "old_content/publication.html#publications-in-portuguese",
    "href": "old_content/publication.html#publications-in-portuguese",
    "title": "Yuri Marca’s Homepage",
    "section": "Publications in Portuguese",
    "text": "Publications in Portuguese\n\nY. P. Marca, S. Scholze. Proposta de Substituição da Comunicação GSM em Smart Grids por Rádios de Longo Alcance. XXXIII Simpósio Brasileiro de Telecomunicações, 2015, Juiz de Fora, MG. Anais Completo da Programação Técnica, 2015.\nY. P. Marca, C. E. A. L. Rocha, B. Schneider Jr , F. K. Schneider. Plataforma de Apoio ao Processo Decisório em Pesquisa e Desenvolvimento Tecnológico em Saúde. Congresso Brasileiro de Engenharia Biomédica, 2012, Porto de Galinhas. ANAIS - CBEB 2012, 2012. (pdf)\nM. P. Krause, D. M. Nakato, Y. P. Marca, F. K. Schneider. Gerenciamento do Controle da Glicemia Utilizando um Aplicativo para Celular. Congresso Brasileiro de Engenharia Biomédica, 2012, Porto de Galinhas. ANAIS - CBEB 2012, 2012."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "👋 Hi there, I’m Yuri Marca",
    "section": "",
    "text": "o I am a Data Scientist with a strong background in machine learning, optimization, and MLOps, focused on developing scalable AI solutions and deploying models into production. I hold a Bachelor’s in Electronic Engineering and a Master’s in Information Systems, with international academic experience across Japan, the United Kingdom, and Canada. This diverse background has equipped me with strong analytical skills, the ability to quickly understand and implement state-of-the-art research, and a scientific approach to solving complex industry challenges.\nIn my industry experience, I have worked extensively with machine learning and deep learning frameworks to train and fine-tune models for predictive modeling, anomaly detection, and computer vision. My expertise in MLOps enables me to design and implement end-to-end pipelines that ensure reproducibility and scalability. Leveraging tools like MLflow, Docker, and cloud platforms like AWS and DigitalOcean, I have successfully deployed machine learning models and integrated them into production via API services.\nWith a strong statistical analysis and optimization foundation, I am particularly interested in Generative AI and LLMs, actively exploring their potential in real-world applications. I am eager to contribute to cutting-edge AI solutions and look forward to collaborating with professionals and organizations driving AI innovation in production environments."
  },
  {
    "objectID": "index.html#technical-expertise",
    "href": "index.html#technical-expertise",
    "title": "👋 Hi there, I’m Yuri Marca",
    "section": "🛠️ Technical Expertise",
    "text": "🛠️ Technical Expertise\n\nOperating Systems:  Linux (Fedora, Ubuntu),  Windows.\nProgramming Languages:  Python,  C,  C++,  Bash (Unix Shell).\nMachine Learning Frameworks:  PyTorch,  scikit-learn,  XGBoost, FastAI.\nMLOps & Deployment Tools:  Docker, MLflow, Hydra,  Git,  CI/CD (GitLab, GitHub Actions),  FastAPI.\nData Engineering & Pipelines:  NumPy,  Pandas, Dask,  PostgreSQL, Redshift, InfluxDB.\nCloud Platforms:  AWS,  DigitalOcean.\nOptimization: Bayesian Optimization, Multi-objective Evolutionary Algorithms."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "👋 Hi there, I’m Yuri Marca",
    "section": "🌟 Projects",
    "text": "🌟 Projects\n\nMusic Genre Classification: Built a machine learning model to classify music genres based on audio features.\nML Pipeline for Short-Term Rental Prices: Designed and implemented a full ML pipeline to predict short-term rental prices, integrating data ingestion, preprocessing, model training, and deployment using industry best practices.\nImage Description using OpenAI: Developed an image captioning model leveraging OpenAI’s API to generate meaningful descriptions for images.\nOCBA-MCTS: Reproduced results from the OCBA-MCTS paper, focusing on optimization in Monte Carlo Tree Search through Optimal Computing Budget Allocation algorithm."
  }
]