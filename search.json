[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Building an End-to-End ML Pipeline\n\n\n\n\n\n\nml pipelines\n\n\nreproducible experiments\n\n\nmlflow\n\n\nwandb\n\n\nhydra\n\n\npython\n\n\n\nProject development is part of the MLOps course from Udacity\n\n\n\n\n\nFeb 9, 2025\n\n\nYuri Marca\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "old_content/about.html",
    "href": "old_content/about.html",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "",
    "text": "Oct 2019 ~ Present ‚Äì Warwick Business School, United Kingdom\n\nDoctor of Philosophy in Business and Management\n\nApr 2017 ~ Mar 2019 ‚Äì Shinshu University, Japan\n\nMaster of Engineering in Electronics and Information System\n\nSept 2012 ~ Aug 2013 ‚Äì Concordia University, Canada\n\nExchange Student, Electrical Engineering\n\nJan 2010 ~ Jul 2016 ‚Äì Federal University of Technology - Parana (UTFPR), Brazil\n\nBachelor of Engineering in Electronics"
  },
  {
    "objectID": "old_content/about.html#education",
    "href": "old_content/about.html#education",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "",
    "text": "Oct 2019 ~ Present ‚Äì Warwick Business School, United Kingdom\n\nDoctor of Philosophy in Business and Management\n\nApr 2017 ~ Mar 2019 ‚Äì Shinshu University, Japan\n\nMaster of Engineering in Electronics and Information System\n\nSept 2012 ~ Aug 2013 ‚Äì Concordia University, Canada\n\nExchange Student, Electrical Engineering\n\nJan 2010 ~ Jul 2016 ‚Äì Federal University of Technology - Parana (UTFPR), Brazil\n\nBachelor of Engineering in Electronics"
  },
  {
    "objectID": "old_content/about.html#research-experience",
    "href": "old_content/about.html#research-experience",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "Research Experience",
    "text": "Research Experience\n\nApr 2017 ~ Mar 2019 ‚Äì Shinshu University, Japan\n\nStudy the impact of difficult Pareto set topologies on the performance of multi-objective evolutionary algorithms (MOEA), and proposal of a new method to improve MOEA‚Äôs performance on such problems.\n\nMay 2013 ~ Aug 2013 ‚Äì Concordia University, Canada\n\nShort research on the use of software filters to decrease signal interferences from eyes and muscles in electroencephalography.\n\nApr 2011 ~ Aug 2012 ‚Äì Federal University of Technology - Parana (UTFPR), Brazil\n\nSupported a master‚Äôs student in business administration on the development of a Windows application that implemented the proposed method of his thesis.\nSupport to a medical research group on the development of an Android OS application for monitoring patients with diabetes."
  },
  {
    "objectID": "old_content/about.html#industry-experience",
    "href": "old_content/about.html#industry-experience",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "Industry Experience",
    "text": "Industry Experience\n\nOct 2014 ~ Dec 2015 ‚Äì Internship at SSE Gridtech, Brazil\nJan 2016 ~ Aug 2016 ‚Äì R&D Engineer at SSE Gridtech, Brazil\n\nEmbedded system development for long-distance communication radio as part of a smart metering solution."
  },
  {
    "objectID": "old_content/about.html#award-and-recognition",
    "href": "old_content/about.html#award-and-recognition",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "Award and Recognition",
    "text": "Award and Recognition\n\nBest Student Paper Award, International Conference on Evolutionary Multi-Criterion Optimization (EMO) 2019.\nYoung Researcher Award, IEEE Computational Intelligence Society Japan Chapter, Japanese Symposium on Evolutionary Computation 2018.\nYoung Research Paper Award, IEICE Shin-etsu 2018 Branch IEEE Session."
  },
  {
    "objectID": "old_content/about.html#technical-expertise",
    "href": "old_content/about.html#technical-expertise",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "Technical Expertise",
    "text": "Technical Expertise\nProgramming Languages: Python, Assembly, C, C++, Java, Bash (Unix Shell)\nEnvironment Tools: MATLAB, R Studio, Android Studio, OrCAD, Quartus II"
  },
  {
    "objectID": "old_content/index.html",
    "href": "old_content/index.html",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "",
    "text": "I am a PhD Student in the Operational Research group of Warwick Business School, U.K. I have earned my Master degree in Electronic and Information Engineering from Shinshu University, Japan, and my Bachelor degree in Electronic Engineering from Federal University of Technology ‚Äì Paran√° (UTFPR), Brazil. During my undergraduate studies, I studied one year at Concordia University, Canada as an exchange student participant of the program Science Without Borders. My research interests include Evolutionary Computation, Multi-objective Optimization, and Operational Research."
  },
  {
    "objectID": "old_content/index.html#summary",
    "href": "old_content/index.html#summary",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "",
    "text": "I am a PhD Student in the Operational Research group of Warwick Business School, U.K. I have earned my Master degree in Electronic and Information Engineering from Shinshu University, Japan, and my Bachelor degree in Electronic Engineering from Federal University of Technology ‚Äì Paran√° (UTFPR), Brazil. During my undergraduate studies, I studied one year at Concordia University, Canada as an exchange student participant of the program Science Without Borders. My research interests include Evolutionary Computation, Multi-objective Optimization, and Operational Research."
  },
  {
    "objectID": "old_content/index.html#news",
    "href": "old_content/index.html#news",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "NEWS",
    "text": "NEWS\n\n2019 November: Participant of the COST Action Training school CA15140 at University of Coimbra, Portugal.\n2019 October: I have started the PhD course at Warwick Business School, United Kingdom.\n2019 July: Our paper has been accepted to be published on Japan Evolutionary Computation Society‚Äôs Journal.\n\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, K. Tanaka: MOEA with Cubic Interpolation on Bi-objective Problems with Difficult Pareto Set Topology\n\n2019 March: I have graduated from Shinshu University, Japan.\n\nThesis‚Äô title: Interpolation Model Based Evolutionary Algorithm to Solve Multi-objective Optimization Problems with Difficult Pareto Set Topology.\n\n2019 March: I received the Best Student Paper Award at EMO2019 for the paper:\n\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, K. Tanaka: Approximating Pareto set topology by cubic interpolation on bi-objective problems.\n\n2018 December: I received the Young Researcher Award from JPNSEC 2018 Symposium on Evolutionary Computation for the paper:\n\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka.: NSGA-II with Spline Interpolation on Bi-objective Problems with Difficult Pareto Set Topology.\n\n\nPast activities"
  },
  {
    "objectID": "old_content/index.html#contact",
    "href": "old_content/index.html#contact",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "Contact",
    "text": "Contact\n\nyurimarca [at] g m a i l [dot] c o m"
  },
  {
    "objectID": "old_content/index.html#online-cvs",
    "href": "old_content/index.html#online-cvs",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "Online CVs",
    "text": "Online CVs\n\nGoogle Scholar\nLinkedIn\nLattes Platform ‚Äì Brazilian government‚Äôs research registry"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Data Scientist with a strong background in machine learning, optimization, and MLOps, focused on developing scalable AI solutions and deploying models into production. I hold a Bachelor‚Äôs in Electronic Engineering and a Master‚Äôs in Information Systems, with international academic experience across Japan, the United Kingdom, and Canada. This diverse background has equipped me with strong analytical skills, the ability to quickly understand and implement state-of-the-art research, and a scientific approach to solving complex industry challenges.\nIn my industry experience, I have worked extensively with machine learning and deep learning frameworks to train and fine-tune models for predictive modeling, anomaly detection, and computer vision. My expertise in MLOps enables me to design and implement end-to-end pipelines that ensure reproducibility and scalability. Leveraging tools like MLflow, Docker, and cloud platforms like AWS and DigitalOcean, I have successfully deployed machine learning models and integrated them into production via API services.\nWith a strong statistical analysis and optimization foundation, I am particularly interested in Generative AI and LLMs, actively exploring their potential in real-world applications. I am eager to contribute to cutting-edge AI solutions and look forward to collaborating with professionals and organizations driving AI innovation in production environments."
  },
  {
    "objectID": "about.html#technical-expertise",
    "href": "about.html#technical-expertise",
    "title": "About",
    "section": "Technical Expertise",
    "text": "Technical Expertise\n\nOperating Systems:\n Linux (Fedora, Ubuntu),  Windows.\n\n\n\nProgramming Languages:\n Python,  C,  C++,  Bash (Unix Shell).\n\n\n\nMachine Learning Frameworks:\n PyTorch,  scikit-learn,  XGBoost, FastAI.\n\n\n\nMLOps & Deployment Tools:\n Docker,  MLflow,  Hydra,  Git,  CI/CD (GitLab, GitHub Actions),  FastAPI.\n\n\n\nData Engineering & Pipelines:\n NumPy,  Pandas, Dask,  PostgreSQL, Redshift, InfluxDB.\n\n\n\nCloud Platforms:\n AWS,  DigitalOcean.\n\n\n\nOptimization:\nBayesian Optimization, Multi-objective Evolutionary Algorithms."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "In the dynamic world of short-term property rentals, accurately predicting rental prices is crucial for maximizing occupancy and revenue. To address this challenge, I developed a comprehensive, reproducible Machine Learning (ML) pipeline tailored for short-term rental price prediction in New York City (NYC) based on Airbnb data. We walk through data collection, cleaning, validation, splitting, training a random forest model, testing the model, and logging every artifact and result in a reproducible manner. This blog post focus on the usage of MLflow, Weights & Biases (W&B), and Hydra for orchestration, tracking, and hyperparameter optimization, as these are taught in the Udacity MLOps Nanodegree program.\n\n\nShort-term rental platforms like Airbnb collect vast amounts of data from hosts and guests. A host often asks: What price should I set for my listing to optimize occupancy and revenue? This project aims to solve that by building an end-to-end ML pipeline to predict a short-term rental‚Äôs price given various features such as location, room type, and number of reviews.\n\nFocus: Clean, reproducible, and easily re-runnable pipeline.\nTools: MLflow for orchestration, Hydra for configuration, Weights & Biases to log artifacts and metrics, and scikit-learn for model training.\nAdditional: ydata-profiling for EDA, custom data checks to prevent ‚Äúdata drift,‚Äù and a templated approach to add new steps using a cookiecutter MLflow step.\n\nThe entire pipeline is tracked under a W&B project to enable experiment tracking and artifact storage. Here is the link for the completed project in W&B.\n\n\n\nThe project is organized as follows:\n.\n‚îú‚îÄ‚îÄ components/                  # Reusable pipeline components\n‚îÇ   ‚îú‚îÄ‚îÄ get_data/                # Data ingestion module\n‚îÇ   ‚îú‚îÄ‚îÄ test_regression_model/   # Model testing module\n‚îÇ   ‚îî‚îÄ‚îÄ train_val_test_split/    # Data splitting module\n‚îú‚îÄ‚îÄ src/                         # Source code for main pipeline steps\n‚îÇ   ‚îú‚îÄ‚îÄ basic_cleaning/          # Data cleaning scripts\n‚îÇ   ‚îú‚îÄ‚îÄ data_check/              # Data validation scripts\n‚îÇ   ‚îú‚îÄ‚îÄ eda/                     # Exploratory Data Analysis scripts\n‚îÇ   ‚îî‚îÄ‚îÄ train_random_forest/     # Model training scripts\n‚îú‚îÄ‚îÄ images/                      # Visual assets for documentation\n‚îú‚îÄ‚îÄ .github/workflows/           # GitHub Actions for CI/CD\n‚îú‚îÄ‚îÄ cookie-mlflow-step/          # Template for creating new pipeline steps\n‚îú‚îÄ‚îÄ conda.yml                    # Conda environment configuration\n‚îú‚îÄ‚îÄ config.yaml                  # Pipeline configuration settings\n‚îú‚îÄ‚îÄ environment.yml              # Alternative environment configuration\n‚îú‚îÄ‚îÄ main.py                      # Main script to run the pipeline\n‚îú‚îÄ‚îÄ MLproject                    # MLflow project specification\n‚îî‚îÄ‚îÄ README.md                    # Project documentation\nKey folders:\n\ncomponents/: Reusable MLflow components for tasks like data downloading, data splitting, and model testing.\n\nsrc/: Specific pipeline steps, including EDA (eda), data cleaning (basic_cleaning), data checks (data_check), and training a random forest (train_random_forest).\n\ncookie-mlflow-step/: A cookiecutter template that quickly scaffolds new MLflow pipeline steps.\n\nmain.py: The orchestrator that references each step through Hydra configuration.\n\nMLproject: Defines how MLflow runs the pipeline, specifying entry points and environment details.\n\n\n\n\n\n\nThis module handles the retrieval of raw data, ensuring it‚Äôs stored in a structured format suitable for downstream processing. It interfaces with data sources, downloads datasets, and logs them as artifacts for version control.\n\n\n\nData cleaning involves:\n\nRemoving duplicates and irrelevant entries.\nHandling missing values through imputation or removal.\nCorrecting data types and formatting issues.\nAddressing outliers to prevent skewed model training.\n\n\n\n\nEDA provides insights into the dataset through:\n\nStatistical summaries of features.\nVisualizations to identify patterns and correlations.\nDetection of anomalies or unexpected distributions.\n\n\n\n\nBefore model training, data validation checks are performed to ensure:\n\nConsistency in data formats and ranges.\nIntegrity constraints are maintained.\nAlignment with expected distributions to prevent data drift.\n\n\n\n\nThe dataset is partitioned into:\n\nTraining Set: For model learning.\nValidation Set: For hyperparameter tuning and model selection.\nTest Set: For final evaluation of model performance.\n\n\n\n\nUtilizing the Random Forest algorithm, this step involves:\n\nTraining the model on the prepared dataset.\nLogging training parameters and metrics.\nSaving the trained model artifact for evaluation and deployment.\n\n\n\n\nThe trained model undergoes rigorous evaluation to assess:\n\nPredictive accuracy on unseen data.\nGeneralization capabilities.\nPotential overfitting or underfitting issues.\n\n\n\n\n\n\nWe rely on two environment files:\n\nenvironment.yml: Sets up the main environment (nyc_airbnb_dev) with Python 3.10, Hydra, Jupyter, and crucial Python packages (MLflow, W&B, ydata-profiling, etc.).\n\nconda.yml (in various subfolders): Each step can be run in a mini environment with the dependencies it needs.\n\nTo install:\nconda env create -f environment.yml\nconda activate nyc_airbnb_dev\nwandb login [your_API_key]\nAfter this, you can run:\nmlflow run . \nMLflow will pick up the MLproject file in the root directory and execute main.py.\n\n\n\n\n\n\nThe code for downloading data is stored in components/get_data. We keep a couple of sample CSVs in data/ that stand in for a real-world dataset. This step simply logs an artifact to W&B.\nCode Snippet from components/get_data/run.py\ndef go(args):\n    run = wandb.init(job_type=\"download_file\")\n    run.config.update(args)\n\n    log_artifact(\n        args.artifact_name,\n        args.artifact_type,\n        args.artifact_description,\n        os.path.join(\"data\", args.sample),\n        run,\n    )\nTo run just the download step:\nmlflow run . -P steps=download\n\n\n\nWe have an EDA folder (src/eda). It contains a EDA.ipynb notebook, which uses ydata-profiling for generating a quick profile report of the dataset. The snippet below shows how we generate an HTML report and log it to W&B.\nCode Snippet from src/eda/EDA.ipynb\nimport ydata_profiling\nprofile = ydata_profiling.ProfileReport(df)\nprofile.to_file(\"profile-report.html\")\nartifact = wandb.Artifact(\n    name=\"profile-report.html\", \n    type=\"analysis\", \n    description=\"Report from ydata-profiling\"\n)\nartifact.add_file(\"profile-report.html\")\nrun.log_artifact(artifact)\nWe also create various plots (like price distribution) and attempt to remove obvious outliers (price &lt;10 or &gt;350) to get a more reasonable dataset.\nKey Observations:\n- Some columns like last_review and reviews_per_month can have many missing values.\n- The distribution of price is highly skewed.\n- We remove out-of-bound lat/long values that are not within the known NYC boundaries.\n\n\n\nA dedicated step in src/basic_cleaning/ cleans the data after EDA reveals certain constraints.\nWe used Cookiecutter to generate the skeleton for this step and then filled in the logic. The run.py file:\nmlflow run . -P steps=basic_cleaning\nCore snippet from src/basic_cleaning/run.py:\ndf = df.drop_duplicates().reset_index(drop=True)\nidx = df['price'].between(args.min_price, args.max_price)\ndf = df[idx].copy()\nidx = df['longitude'].between(-74.25, -73.50) & df['latitude'].between(40.5, 41.2)\ndf = df[idx].copy()\n\n# Log the cleaned CSV as W&B artifact\nartifact = wandb.Artifact(\n    name=args.output_artifact,\n    type=args.output_type,\n    description=args.output_description,\n)\nartifact.add_file(fp.name)\nrun.log_artifact(artifact)\nIt:\n1. Drops duplicates.\n2. Filters out out-of-range prices.\n3. Ensures lat/long within valid NYC boundary.\n4. Logs the cleaned dataset to W&B.\n\n\n\nWe follow the concept of Data Testing to guard against ‚Äúdata pipeline rot.‚Äù The step is in src/data_check/. It uses pytest tests to verify that the cleaned data:\n\nHas valid column names.\nFalls within expected lat/long boundaries.\nContains only known neighborhoods.\nDistributes similarly to a reference dataset (via KL divergence).\nRespects a minimum and maximum price range.\n\nA snippet of the test suite from src/data_check/test_data.py:\ndef test_similar_neigh_distrib(data: pd.DataFrame, ref_data: pd.DataFrame, kl_threshold: float):\n    dist1 = data['neighbourhood_group'].value_counts().sort_index()\n    dist2 = ref_data['neighbourhood_group'].value_counts().sort_index()\n    assert scipy.stats.entropy(dist1, dist2, base=2) &lt; kl_threshold\nTo run:\nmlflow run . -P steps=data_check\nAny mismatches or anomalies raise an exception that stops the pipeline, keeping you from training on ‚Äúbad‚Äù data.\n\n\n\nWe then split our dataset into training, validation, and test sets (the last set is strictly for final model testing). The relevant code is in components/train_val_test_split/.\nCode Snippet from components/train_val_test_split/run.py\ntrainval, test = train_test_split(\n    df,\n    test_size=args.test_size,\n    random_state=args.random_seed,\n    stratify=df[args.stratify_by] if args.stratify_by != 'none' else None,\n)\nBoth the training/validation split (‚Äútrainval_data.csv‚Äù) and the test split (‚Äútest_data.csv‚Äù) are then logged to W&B.\n\n\n\nWith a clean train-validation dataset, we build a random forest pipeline in src/train_random_forest/run.py. This step is heavily reliant on Hydra for configuration. We define parameters like max_depth, n_estimators, etc., in config.yaml.\nHere‚Äôs a highlight from run.py:\ndef get_inference_pipeline(rf_config, max_tfidf_features):\n    ordinal_categorical = [\"room_type\"]\n    non_ordinal_categorical = [\"neighbourhood_group\"]\n    \n    ordinal_categorical_preproc = OrdinalEncoder()\n    non_ordinal_categorical_preproc = Pipeline([\n        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"encode\", OneHotEncoder())\n    ])\n    ...\n    random_forest = RandomForestRegressor(**rf_config)\n    sk_pipe = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"random_forest\", random_forest),\n    ])\n    return sk_pipe, processed_features\nIn this pipeline, we handle:\n- Categorical columns (OrdinalEncoder or OneHotEncoder).\n- Numerical columns (imputation for missing values).\n- NLP on the name field using a TF-IDF vectorizer.\nWe train and evaluate on a validation set, logging all metrics (MAE, R^2) to W&B. Finally, the pipeline (preprocessing + model) is saved to MLflow format and uploaded to W&B as an artifact:\nmlflow.sklearn.save_model(\n    sk_pipe,\n    export_path,\n    serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE,\n    signature=sig,\n    input_example=X_val[processed_features].iloc[:2]\n)\nBy storing the entire pipeline, we can apply transformations consistently at inference time.\nTo run the model training step:\nmlflow run . -P steps=train_random_forest\n\n\n\nLastly, we evaluate our finalized model against the test set. The code is in components/test_regression_model/run.py. It:\n\nDownloads the prod model artifact from W&B.\n\nLoads the test dataset.\n\nGenerates predictions and calculates R^2 and MAE.\n\nLogs test performance to W&B.\n\ndef go(args):\n    sk_pipe = mlflow.sklearn.load_model(model_local_path)\n    y_pred = sk_pipe.predict(X_test)\n    r_squared = sk_pipe.score(X_test, y_test)\n    mae = mean_absolute_error(y_test, y_pred)\n\n    run.summary['r2'] = r_squared\n    run.summary['mae'] = mae\nYou only run this step after a model has been tagged for production, ensuring it has proven to be stable in dev/validation.\n\n\n\n\n\nThe main.py script orchestrates the entire flow. It reads config.yaml via Hydra and decides which steps to run based on a comma-separated list. A minimal snippet:\n_steps = [\n    \"download\",\n    \"basic_cleaning\",\n    \"data_check\",\n    \"data_split\",\n    \"train_random_forest\",\n    # \"test_regression_model\" is triggered separately\n]\n\n@hydra.main(config_name='config')\ndef go(config: DictConfig):\n\n    steps_par = config['main']['steps']\n    active_steps = steps_par.split(\",\") if steps_par != \"all\" else _steps\n\n    if \"download\" in active_steps:\n        mlflow.run(\n            f\"{config['main']['components_repository']}/get_data\",\n            ...\n        )\n\n    if \"basic_cleaning\" in active_steps:\n        mlflow.run(\n            os.path.join(hydra.utils.get_original_cwd(), \"src\", \"basic_cleaning\"),\n            ...\n        )\n\n    ...\nTo execute the entire pipeline:\nmlflow run . \nOr pick and choose steps:\nmlflow run . -P steps=download,basic_cleaning,data_check\n\n\n\n\nOne of the repository‚Äôs highlights is the cookie-mlflow-step folder, which speeds up adding new steps to the pipeline:\ncookiecutter cookie-mlflow-step -o src\nIt scaffolds a new directory structure containing an MLproject, conda.yml, and a run.py pre-populated with arguments. This is helpful for consistent, standardized pipeline steps where each step is an MLflow project.\n\n\n\n\n\nCI/CD: The .github/workflows/manual.yml shows how a manual GH Action can create Jira tickets for new Pull Requests.\n\nRelease: Tagging your repo with a version number (e.g., 1.0.0) allows you to run the pipeline from a specific commit. For instance:\nmlflow run https://github.com/yurimarca/build-ml-pipeline-for-short-term-rental-prices.git \\\n  -v 1.0.0 \\\n  -P hydra_options=\"etl.sample='sample2.csv'\"\nArtifact Logging: W&B captures every CSV and model artifact, so future steps or collaborators can trace lineage and retrieve them easily.\n\n\n\n\n\nThis repository combines Hydra for flexible configuration, MLflow for pipeline orchestration, and Weights & Biases for experiment tracking to create a fully reproducible short-term rental price prediction pipeline in NYC.\nKey Takeaways:\n\nData integrity: By integrating data validation tests, the pipeline can fail early if data is incorrect.\n\nReproducible training: Using Hydra, MLflow, and environment files ensures consistent environments and parameter definitions.\n\nFull pipeline tracking: From EDA to final test, each artifact is logged to W&B, making it easy to revert or compare different runs.\n\nExtensibility: The cookiecutter approach helps you quickly add new pipeline steps or replicate the same pipeline structure in other projects.\n\nFeel free to explore the code base, and don‚Äôt hesitate to experiment by customizing steps or hyperparameters. By following this pipeline, you can keep your machine learning workflow tidy, versioned, and production-ready.\nFor a detailed walkthrough and access to the codebase, visit the GitHub repository.\nNote: This project was developed as part of the Udacity MLOps Nanodegree program."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#project-overview",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#project-overview",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "Short-term rental platforms like Airbnb collect vast amounts of data from hosts and guests. A host often asks: What price should I set for my listing to optimize occupancy and revenue? This project aims to solve that by building an end-to-end ML pipeline to predict a short-term rental‚Äôs price given various features such as location, room type, and number of reviews.\n\nFocus: Clean, reproducible, and easily re-runnable pipeline.\nTools: MLflow for orchestration, Hydra for configuration, Weights & Biases to log artifacts and metrics, and scikit-learn for model training.\nAdditional: ydata-profiling for EDA, custom data checks to prevent ‚Äúdata drift,‚Äù and a templated approach to add new steps using a cookiecutter MLflow step.\n\nThe entire pipeline is tracked under a W&B project to enable experiment tracking and artifact storage. Here is the link for the completed project in W&B."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#repository-structure",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#repository-structure",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "The project is organized as follows:\n.\n‚îú‚îÄ‚îÄ components/                  # Reusable pipeline components\n‚îÇ   ‚îú‚îÄ‚îÄ get_data/                # Data ingestion module\n‚îÇ   ‚îú‚îÄ‚îÄ test_regression_model/   # Model testing module\n‚îÇ   ‚îî‚îÄ‚îÄ train_val_test_split/    # Data splitting module\n‚îú‚îÄ‚îÄ src/                         # Source code for main pipeline steps\n‚îÇ   ‚îú‚îÄ‚îÄ basic_cleaning/          # Data cleaning scripts\n‚îÇ   ‚îú‚îÄ‚îÄ data_check/              # Data validation scripts\n‚îÇ   ‚îú‚îÄ‚îÄ eda/                     # Exploratory Data Analysis scripts\n‚îÇ   ‚îî‚îÄ‚îÄ train_random_forest/     # Model training scripts\n‚îú‚îÄ‚îÄ images/                      # Visual assets for documentation\n‚îú‚îÄ‚îÄ .github/workflows/           # GitHub Actions for CI/CD\n‚îú‚îÄ‚îÄ cookie-mlflow-step/          # Template for creating new pipeline steps\n‚îú‚îÄ‚îÄ conda.yml                    # Conda environment configuration\n‚îú‚îÄ‚îÄ config.yaml                  # Pipeline configuration settings\n‚îú‚îÄ‚îÄ environment.yml              # Alternative environment configuration\n‚îú‚îÄ‚îÄ main.py                      # Main script to run the pipeline\n‚îú‚îÄ‚îÄ MLproject                    # MLflow project specification\n‚îî‚îÄ‚îÄ README.md                    # Project documentation\nKey folders:\n\ncomponents/: Reusable MLflow components for tasks like data downloading, data splitting, and model testing.\n\nsrc/: Specific pipeline steps, including EDA (eda), data cleaning (basic_cleaning), data checks (data_check), and training a random forest (train_random_forest).\n\ncookie-mlflow-step/: A cookiecutter template that quickly scaffolds new MLflow pipeline steps.\n\nmain.py: The orchestrator that references each step through Hydra configuration.\n\nMLproject: Defines how MLflow runs the pipeline, specifying entry points and environment details."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#pipeline-components",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#pipeline-components",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "This module handles the retrieval of raw data, ensuring it‚Äôs stored in a structured format suitable for downstream processing. It interfaces with data sources, downloads datasets, and logs them as artifacts for version control.\n\n\n\nData cleaning involves:\n\nRemoving duplicates and irrelevant entries.\nHandling missing values through imputation or removal.\nCorrecting data types and formatting issues.\nAddressing outliers to prevent skewed model training.\n\n\n\n\nEDA provides insights into the dataset through:\n\nStatistical summaries of features.\nVisualizations to identify patterns and correlations.\nDetection of anomalies or unexpected distributions.\n\n\n\n\nBefore model training, data validation checks are performed to ensure:\n\nConsistency in data formats and ranges.\nIntegrity constraints are maintained.\nAlignment with expected distributions to prevent data drift.\n\n\n\n\nThe dataset is partitioned into:\n\nTraining Set: For model learning.\nValidation Set: For hyperparameter tuning and model selection.\nTest Set: For final evaluation of model performance.\n\n\n\n\nUtilizing the Random Forest algorithm, this step involves:\n\nTraining the model on the prepared dataset.\nLogging training parameters and metrics.\nSaving the trained model artifact for evaluation and deployment.\n\n\n\n\nThe trained model undergoes rigorous evaluation to assess:\n\nPredictive accuracy on unseen data.\nGeneralization capabilities.\nPotential overfitting or underfitting issues."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#setting-up-the-environment",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#setting-up-the-environment",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "We rely on two environment files:\n\nenvironment.yml: Sets up the main environment (nyc_airbnb_dev) with Python 3.10, Hydra, Jupyter, and crucial Python packages (MLflow, W&B, ydata-profiling, etc.).\n\nconda.yml (in various subfolders): Each step can be run in a mini environment with the dependencies it needs.\n\nTo install:\nconda env create -f environment.yml\nconda activate nyc_airbnb_dev\nwandb login [your_API_key]\nAfter this, you can run:\nmlflow run . \nMLflow will pick up the MLproject file in the root directory and execute main.py."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#pipeline-steps",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#pipeline-steps",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "The code for downloading data is stored in components/get_data. We keep a couple of sample CSVs in data/ that stand in for a real-world dataset. This step simply logs an artifact to W&B.\nCode Snippet from components/get_data/run.py\ndef go(args):\n    run = wandb.init(job_type=\"download_file\")\n    run.config.update(args)\n\n    log_artifact(\n        args.artifact_name,\n        args.artifact_type,\n        args.artifact_description,\n        os.path.join(\"data\", args.sample),\n        run,\n    )\nTo run just the download step:\nmlflow run . -P steps=download\n\n\n\nWe have an EDA folder (src/eda). It contains a EDA.ipynb notebook, which uses ydata-profiling for generating a quick profile report of the dataset. The snippet below shows how we generate an HTML report and log it to W&B.\nCode Snippet from src/eda/EDA.ipynb\nimport ydata_profiling\nprofile = ydata_profiling.ProfileReport(df)\nprofile.to_file(\"profile-report.html\")\nartifact = wandb.Artifact(\n    name=\"profile-report.html\", \n    type=\"analysis\", \n    description=\"Report from ydata-profiling\"\n)\nartifact.add_file(\"profile-report.html\")\nrun.log_artifact(artifact)\nWe also create various plots (like price distribution) and attempt to remove obvious outliers (price &lt;10 or &gt;350) to get a more reasonable dataset.\nKey Observations:\n- Some columns like last_review and reviews_per_month can have many missing values.\n- The distribution of price is highly skewed.\n- We remove out-of-bound lat/long values that are not within the known NYC boundaries.\n\n\n\nA dedicated step in src/basic_cleaning/ cleans the data after EDA reveals certain constraints.\nWe used Cookiecutter to generate the skeleton for this step and then filled in the logic. The run.py file:\nmlflow run . -P steps=basic_cleaning\nCore snippet from src/basic_cleaning/run.py:\ndf = df.drop_duplicates().reset_index(drop=True)\nidx = df['price'].between(args.min_price, args.max_price)\ndf = df[idx].copy()\nidx = df['longitude'].between(-74.25, -73.50) & df['latitude'].between(40.5, 41.2)\ndf = df[idx].copy()\n\n# Log the cleaned CSV as W&B artifact\nartifact = wandb.Artifact(\n    name=args.output_artifact,\n    type=args.output_type,\n    description=args.output_description,\n)\nartifact.add_file(fp.name)\nrun.log_artifact(artifact)\nIt:\n1. Drops duplicates.\n2. Filters out out-of-range prices.\n3. Ensures lat/long within valid NYC boundary.\n4. Logs the cleaned dataset to W&B.\n\n\n\nWe follow the concept of Data Testing to guard against ‚Äúdata pipeline rot.‚Äù The step is in src/data_check/. It uses pytest tests to verify that the cleaned data:\n\nHas valid column names.\nFalls within expected lat/long boundaries.\nContains only known neighborhoods.\nDistributes similarly to a reference dataset (via KL divergence).\nRespects a minimum and maximum price range.\n\nA snippet of the test suite from src/data_check/test_data.py:\ndef test_similar_neigh_distrib(data: pd.DataFrame, ref_data: pd.DataFrame, kl_threshold: float):\n    dist1 = data['neighbourhood_group'].value_counts().sort_index()\n    dist2 = ref_data['neighbourhood_group'].value_counts().sort_index()\n    assert scipy.stats.entropy(dist1, dist2, base=2) &lt; kl_threshold\nTo run:\nmlflow run . -P steps=data_check\nAny mismatches or anomalies raise an exception that stops the pipeline, keeping you from training on ‚Äúbad‚Äù data.\n\n\n\nWe then split our dataset into training, validation, and test sets (the last set is strictly for final model testing). The relevant code is in components/train_val_test_split/.\nCode Snippet from components/train_val_test_split/run.py\ntrainval, test = train_test_split(\n    df,\n    test_size=args.test_size,\n    random_state=args.random_seed,\n    stratify=df[args.stratify_by] if args.stratify_by != 'none' else None,\n)\nBoth the training/validation split (‚Äútrainval_data.csv‚Äù) and the test split (‚Äútest_data.csv‚Äù) are then logged to W&B.\n\n\n\nWith a clean train-validation dataset, we build a random forest pipeline in src/train_random_forest/run.py. This step is heavily reliant on Hydra for configuration. We define parameters like max_depth, n_estimators, etc., in config.yaml.\nHere‚Äôs a highlight from run.py:\ndef get_inference_pipeline(rf_config, max_tfidf_features):\n    ordinal_categorical = [\"room_type\"]\n    non_ordinal_categorical = [\"neighbourhood_group\"]\n    \n    ordinal_categorical_preproc = OrdinalEncoder()\n    non_ordinal_categorical_preproc = Pipeline([\n        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"encode\", OneHotEncoder())\n    ])\n    ...\n    random_forest = RandomForestRegressor(**rf_config)\n    sk_pipe = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"random_forest\", random_forest),\n    ])\n    return sk_pipe, processed_features\nIn this pipeline, we handle:\n- Categorical columns (OrdinalEncoder or OneHotEncoder).\n- Numerical columns (imputation for missing values).\n- NLP on the name field using a TF-IDF vectorizer.\nWe train and evaluate on a validation set, logging all metrics (MAE, R^2) to W&B. Finally, the pipeline (preprocessing + model) is saved to MLflow format and uploaded to W&B as an artifact:\nmlflow.sklearn.save_model(\n    sk_pipe,\n    export_path,\n    serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE,\n    signature=sig,\n    input_example=X_val[processed_features].iloc[:2]\n)\nBy storing the entire pipeline, we can apply transformations consistently at inference time.\nTo run the model training step:\nmlflow run . -P steps=train_random_forest\n\n\n\nLastly, we evaluate our finalized model against the test set. The code is in components/test_regression_model/run.py. It:\n\nDownloads the prod model artifact from W&B.\n\nLoads the test dataset.\n\nGenerates predictions and calculates R^2 and MAE.\n\nLogs test performance to W&B.\n\ndef go(args):\n    sk_pipe = mlflow.sklearn.load_model(model_local_path)\n    y_pred = sk_pipe.predict(X_test)\n    r_squared = sk_pipe.score(X_test, y_test)\n    mae = mean_absolute_error(y_test, y_pred)\n\n    run.summary['r2'] = r_squared\n    run.summary['mae'] = mae\nYou only run this step after a model has been tagged for production, ensuring it has proven to be stable in dev/validation."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#putting-it-all-together-main.py",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#putting-it-all-together-main.py",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "The main.py script orchestrates the entire flow. It reads config.yaml via Hydra and decides which steps to run based on a comma-separated list. A minimal snippet:\n_steps = [\n    \"download\",\n    \"basic_cleaning\",\n    \"data_check\",\n    \"data_split\",\n    \"train_random_forest\",\n    # \"test_regression_model\" is triggered separately\n]\n\n@hydra.main(config_name='config')\ndef go(config: DictConfig):\n\n    steps_par = config['main']['steps']\n    active_steps = steps_par.split(\",\") if steps_par != \"all\" else _steps\n\n    if \"download\" in active_steps:\n        mlflow.run(\n            f\"{config['main']['components_repository']}/get_data\",\n            ...\n        )\n\n    if \"basic_cleaning\" in active_steps:\n        mlflow.run(\n            os.path.join(hydra.utils.get_original_cwd(), \"src\", \"basic_cleaning\"),\n            ...\n        )\n\n    ...\nTo execute the entire pipeline:\nmlflow run . \nOr pick and choose steps:\nmlflow run . -P steps=download,basic_cleaning,data_check"
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#using-cookiecutter-for-quick-pipeline-steps",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#using-cookiecutter-for-quick-pipeline-steps",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "One of the repository‚Äôs highlights is the cookie-mlflow-step folder, which speeds up adding new steps to the pipeline:\ncookiecutter cookie-mlflow-step -o src\nIt scaffolds a new directory structure containing an MLproject, conda.yml, and a run.py pre-populated with arguments. This is helpful for consistent, standardized pipeline steps where each step is an MLflow project."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#deployment-and-workflow",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#deployment-and-workflow",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "CI/CD: The .github/workflows/manual.yml shows how a manual GH Action can create Jira tickets for new Pull Requests.\n\nRelease: Tagging your repo with a version number (e.g., 1.0.0) allows you to run the pipeline from a specific commit. For instance:\nmlflow run https://github.com/yurimarca/build-ml-pipeline-for-short-term-rental-prices.git \\\n  -v 1.0.0 \\\n  -P hydra_options=\"etl.sample='sample2.csv'\"\nArtifact Logging: W&B captures every CSV and model artifact, so future steps or collaborators can trace lineage and retrieve them easily."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#conclusion",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#conclusion",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "This repository combines Hydra for flexible configuration, MLflow for pipeline orchestration, and Weights & Biases for experiment tracking to create a fully reproducible short-term rental price prediction pipeline in NYC.\nKey Takeaways:\n\nData integrity: By integrating data validation tests, the pipeline can fail early if data is incorrect.\n\nReproducible training: Using Hydra, MLflow, and environment files ensures consistent environments and parameter definitions.\n\nFull pipeline tracking: From EDA to final test, each artifact is logged to W&B, making it easy to revert or compare different runs.\n\nExtensibility: The cookiecutter approach helps you quickly add new pipeline steps or replicate the same pipeline structure in other projects.\n\nFeel free to explore the code base, and don‚Äôt hesitate to experiment by customizing steps or hyperparameters. By following this pipeline, you can keep your machine learning workflow tidy, versioned, and production-ready.\nFor a detailed walkthrough and access to the codebase, visit the GitHub repository.\nNote: This project was developed as part of the Udacity MLOps Nanodegree program."
  },
  {
    "objectID": "old_content/publication.html",
    "href": "old_content/publication.html",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "",
    "text": "Y. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. Approximating Pareto set topology by cubic interpolation on bi-objective problems. 10th International Conference on Evolutionary Multi-Criterion Optimization (EMO2019), Lecture Notes in Computer Science (LNCS), vol 11411, pp 386-398, East Lansing, Michigan, USA, 2019 DOI ‚òÖBest Student Paper Award‚òÖ\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. NSGA-II with Spline Interpolation on Bi-objective Problems with Difficult Pareto Set Topology. JPNSEC 2018 Symposium on Evolutionary Computation, Fukuoka, 2018. ‚òÖYoung Research Award‚òÖ\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. MOEAs on Problems with Difficult Pareto Set Topologies. IEICE Shin-etsu Branch IEEE Session, Niigata University, 2018, p.¬†169. (presentation) ‚òÖYoung Research Paper Award‚òÖ\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. Pareto dominance-based MOEAs on problems with difficult pareto set topologies. In Proceedings of the Genetic and Evolutionary Computation Conference Companion (GECCO ‚Äô18). ACM, New York, NY, USA, 189-190. DOI.\nC. E. A. L. Rocha, Y. P. Marca, F. K. Schneider. Support Platform for Decision-Making in Research and Technological Development in Public Health. ESPACIOS (CARACAS), v. 39, p.¬†14-26, 2018. (link)"
  },
  {
    "objectID": "old_content/publication.html#publications",
    "href": "old_content/publication.html#publications",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "",
    "text": "Y. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. Approximating Pareto set topology by cubic interpolation on bi-objective problems. 10th International Conference on Evolutionary Multi-Criterion Optimization (EMO2019), Lecture Notes in Computer Science (LNCS), vol 11411, pp 386-398, East Lansing, Michigan, USA, 2019 DOI ‚òÖBest Student Paper Award‚òÖ\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. NSGA-II with Spline Interpolation on Bi-objective Problems with Difficult Pareto Set Topology. JPNSEC 2018 Symposium on Evolutionary Computation, Fukuoka, 2018. ‚òÖYoung Research Award‚òÖ\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. MOEAs on Problems with Difficult Pareto Set Topologies. IEICE Shin-etsu Branch IEEE Session, Niigata University, 2018, p.¬†169. (presentation) ‚òÖYoung Research Paper Award‚òÖ\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. Pareto dominance-based MOEAs on problems with difficult pareto set topologies. In Proceedings of the Genetic and Evolutionary Computation Conference Companion (GECCO ‚Äô18). ACM, New York, NY, USA, 189-190. DOI.\nC. E. A. L. Rocha, Y. P. Marca, F. K. Schneider. Support Platform for Decision-Making in Research and Technological Development in Public Health. ESPACIOS (CARACAS), v. 39, p.¬†14-26, 2018. (link)"
  },
  {
    "objectID": "old_content/publication.html#publications-in-portuguese",
    "href": "old_content/publication.html#publications-in-portuguese",
    "title": "Yuri Marca‚Äôs Homepage",
    "section": "Publications in Portuguese",
    "text": "Publications in Portuguese\n\nY. P. Marca, S. Scholze. Proposta de Substitui√ß√£o da Comunica√ß√£o GSM em Smart Grids por R√°dios de Longo Alcance. XXXIII Simp√≥sio Brasileiro de Telecomunica√ß√µes, 2015, Juiz de Fora, MG. Anais Completo da Programa√ß√£o T√©cnica, 2015.\nY. P. Marca, C. E. A. L. Rocha, B. Schneider Jr , F. K. Schneider. Plataforma de Apoio ao Processo Decis√≥rio em Pesquisa e Desenvolvimento Tecnol√≥gico em Sa√∫de. Congresso Brasileiro de Engenharia Biom√©dica, 2012, Porto de Galinhas. ANAIS - CBEB 2012, 2012. (pdf)\nM. P. Krause, D. M. Nakato, Y. P. Marca, F. K. Schneider. Gerenciamento do Controle da Glicemia Utilizando um Aplicativo para Celular. Congresso Brasileiro de Engenharia Biom√©dica, 2012, Porto de Galinhas. ANAIS - CBEB 2012, 2012."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üëã Hi there, I‚Äôm Yuri Marca",
    "section": "",
    "text": "I am a Data Scientist with a strong background in machine learning, optimization, and MLOps, focused on developing scalable AI solutions and deploying models into production. I hold a Bachelor‚Äôs in Electronic Engineering and a Master‚Äôs in Information Systems, with international academic experience across Japan, the United Kingdom, and Canada. This diverse background has equipped me with strong analytical skills, the ability to quickly understand and implement state-of-the-art research, and a scientific approach to solving complex industry challenges.\nIn my industry experience, I have worked extensively with machine learning and deep learning frameworks to train and fine-tune models for predictive modeling, anomaly detection, and computer vision. My expertise in MLOps enables me to design and implement end-to-end pipelines that ensure reproducibility and scalability. Leveraging tools like MLflow, Docker, and cloud platforms like AWS and DigitalOcean, I have successfully deployed machine learning models and integrated them into production via API services.\nWith a strong statistical analysis and optimization foundation, I am particularly interested in Generative AI and LLMs, actively exploring their potential in real-world applications. I am eager to contribute to cutting-edge AI solutions and look forward to collaborating with professionals and organizations driving AI innovation in production environments."
  },
  {
    "objectID": "index.html#technical-expertise",
    "href": "index.html#technical-expertise",
    "title": "üëã Hi there, I‚Äôm Yuri Marca",
    "section": "üõ†Ô∏è Technical Expertise",
    "text": "üõ†Ô∏è Technical Expertise\n\nOperating Systems:  Linux (Fedora, Ubuntu),  Windows.\nProgramming Languages:  Python,  C,  C++,  Bash (Unix Shell).\nMachine Learning Frameworks:  PyTorch,  scikit-learn,  XGBoost, FastAI.\nMLOps & Deployment Tools:  Docker, MLflow, Hydra,  Git,  CI/CD (GitLab, GitHub Actions),  FastAPI.\nData Engineering & Pipelines:  NumPy,  Pandas, Dask,  PostgreSQL, Redshift, InfluxDB.\nCloud Platforms:  AWS,  DigitalOcean.\nOptimization: Bayesian Optimization, Multi-objective Evolutionary Algorithms."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "üëã Hi there, I‚Äôm Yuri Marca",
    "section": "üåü Projects",
    "text": "üåü Projects\n\nMusic Genre Classification: Built a machine learning model to classify music genres based on audio features.\nML Pipeline for Short-Term Rental Prices: Designed and implemented a full ML pipeline to predict short-term rental prices, integrating data ingestion, preprocessing, model training, and deployment using industry best practices.\nImage Description using OpenAI: Developed an image captioning model leveraging OpenAI‚Äôs API to generate meaningful descriptions for images.\nOCBA-MCTS: Reproduced results from the OCBA-MCTS paper, focusing on optimization in Monte Carlo Tree Search through Optimal Computing Budget Allocation algorithm."
  }
]