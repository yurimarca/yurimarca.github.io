[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Classify Time-Series Faults with Pytorch\n\n\n\n\n\n\npytorch\n\n\nMLP\n\n\nNeural Networks\n\n\npreprocessing\n\n\npython\n\n\n\nPreprocessing and MLP training to binary classification\n\n\n\n\n\nFeb 16, 2025\n\n\nYuri Marca\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an End-to-End ML Pipeline\n\n\n\n\n\n\nml pipelines\n\n\nreproducible experiments\n\n\nmlflow\n\n\nwandb\n\n\nhydra\n\n\npython\n\n\n\nProject development is part of the MLOps course from Udacity\n\n\n\n\n\nFeb 9, 2025\n\n\nYuri Marca\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "old_content/about.html",
    "href": "old_content/about.html",
    "title": "Yuri Marca’s Homepage",
    "section": "",
    "text": "Oct 2019 ~ Present – Warwick Business School, United Kingdom\n\nDoctor of Philosophy in Business and Management\n\nApr 2017 ~ Mar 2019 – Shinshu University, Japan\n\nMaster of Engineering in Electronics and Information System\n\nSept 2012 ~ Aug 2013 – Concordia University, Canada\n\nExchange Student, Electrical Engineering\n\nJan 2010 ~ Jul 2016 – Federal University of Technology - Parana (UTFPR), Brazil\n\nBachelor of Engineering in Electronics"
  },
  {
    "objectID": "old_content/about.html#education",
    "href": "old_content/about.html#education",
    "title": "Yuri Marca’s Homepage",
    "section": "",
    "text": "Oct 2019 ~ Present – Warwick Business School, United Kingdom\n\nDoctor of Philosophy in Business and Management\n\nApr 2017 ~ Mar 2019 – Shinshu University, Japan\n\nMaster of Engineering in Electronics and Information System\n\nSept 2012 ~ Aug 2013 – Concordia University, Canada\n\nExchange Student, Electrical Engineering\n\nJan 2010 ~ Jul 2016 – Federal University of Technology - Parana (UTFPR), Brazil\n\nBachelor of Engineering in Electronics"
  },
  {
    "objectID": "old_content/about.html#research-experience",
    "href": "old_content/about.html#research-experience",
    "title": "Yuri Marca’s Homepage",
    "section": "Research Experience",
    "text": "Research Experience\n\nApr 2017 ~ Mar 2019 – Shinshu University, Japan\n\nStudy the impact of difficult Pareto set topologies on the performance of multi-objective evolutionary algorithms (MOEA), and proposal of a new method to improve MOEA’s performance on such problems.\n\nMay 2013 ~ Aug 2013 – Concordia University, Canada\n\nShort research on the use of software filters to decrease signal interferences from eyes and muscles in electroencephalography.\n\nApr 2011 ~ Aug 2012 – Federal University of Technology - Parana (UTFPR), Brazil\n\nSupported a master’s student in business administration on the development of a Windows application that implemented the proposed method of his thesis.\nSupport to a medical research group on the development of an Android OS application for monitoring patients with diabetes."
  },
  {
    "objectID": "old_content/about.html#industry-experience",
    "href": "old_content/about.html#industry-experience",
    "title": "Yuri Marca’s Homepage",
    "section": "Industry Experience",
    "text": "Industry Experience\n\nOct 2014 ~ Dec 2015 – Internship at SSE Gridtech, Brazil\nJan 2016 ~ Aug 2016 – R&D Engineer at SSE Gridtech, Brazil\n\nEmbedded system development for long-distance communication radio as part of a smart metering solution."
  },
  {
    "objectID": "old_content/about.html#award-and-recognition",
    "href": "old_content/about.html#award-and-recognition",
    "title": "Yuri Marca’s Homepage",
    "section": "Award and Recognition",
    "text": "Award and Recognition\n\nBest Student Paper Award, International Conference on Evolutionary Multi-Criterion Optimization (EMO) 2019.\nYoung Researcher Award, IEEE Computational Intelligence Society Japan Chapter, Japanese Symposium on Evolutionary Computation 2018.\nYoung Research Paper Award, IEICE Shin-etsu 2018 Branch IEEE Session."
  },
  {
    "objectID": "old_content/about.html#technical-expertise",
    "href": "old_content/about.html#technical-expertise",
    "title": "Yuri Marca’s Homepage",
    "section": "Technical Expertise",
    "text": "Technical Expertise\nProgramming Languages: Python, Assembly, C, C++, Java, Bash (Unix Shell)\nEnvironment Tools: MATLAB, R Studio, Android Studio, OrCAD, Quartus II"
  },
  {
    "objectID": "old_content/index.html",
    "href": "old_content/index.html",
    "title": "Yuri Marca’s Homepage",
    "section": "",
    "text": "I am a PhD Student in the Operational Research group of Warwick Business School, U.K. I have earned my Master degree in Electronic and Information Engineering from Shinshu University, Japan, and my Bachelor degree in Electronic Engineering from Federal University of Technology – Paraná (UTFPR), Brazil. During my undergraduate studies, I studied one year at Concordia University, Canada as an exchange student participant of the program Science Without Borders. My research interests include Evolutionary Computation, Multi-objective Optimization, and Operational Research."
  },
  {
    "objectID": "old_content/index.html#summary",
    "href": "old_content/index.html#summary",
    "title": "Yuri Marca’s Homepage",
    "section": "",
    "text": "I am a PhD Student in the Operational Research group of Warwick Business School, U.K. I have earned my Master degree in Electronic and Information Engineering from Shinshu University, Japan, and my Bachelor degree in Electronic Engineering from Federal University of Technology – Paraná (UTFPR), Brazil. During my undergraduate studies, I studied one year at Concordia University, Canada as an exchange student participant of the program Science Without Borders. My research interests include Evolutionary Computation, Multi-objective Optimization, and Operational Research."
  },
  {
    "objectID": "old_content/index.html#news",
    "href": "old_content/index.html#news",
    "title": "Yuri Marca’s Homepage",
    "section": "NEWS",
    "text": "NEWS\n\n2019 November: Participant of the COST Action Training school CA15140 at University of Coimbra, Portugal.\n2019 October: I have started the PhD course at Warwick Business School, United Kingdom.\n2019 July: Our paper has been accepted to be published on Japan Evolutionary Computation Society’s Journal.\n\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, K. Tanaka: MOEA with Cubic Interpolation on Bi-objective Problems with Difficult Pareto Set Topology\n\n2019 March: I have graduated from Shinshu University, Japan.\n\nThesis’ title: Interpolation Model Based Evolutionary Algorithm to Solve Multi-objective Optimization Problems with Difficult Pareto Set Topology.\n\n2019 March: I received the Best Student Paper Award at EMO2019 for the paper:\n\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, K. Tanaka: Approximating Pareto set topology by cubic interpolation on bi-objective problems.\n\n2018 December: I received the Young Researcher Award from JPNSEC 2018 Symposium on Evolutionary Computation for the paper:\n\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka.: NSGA-II with Spline Interpolation on Bi-objective Problems with Difficult Pareto Set Topology.\n\n\nPast activities"
  },
  {
    "objectID": "old_content/index.html#contact",
    "href": "old_content/index.html#contact",
    "title": "Yuri Marca’s Homepage",
    "section": "Contact",
    "text": "Contact\n\nyurimarca [at] g m a i l [dot] c o m"
  },
  {
    "objectID": "old_content/index.html#online-cvs",
    "href": "old_content/index.html#online-cvs",
    "title": "Yuri Marca’s Homepage",
    "section": "Online CVs",
    "text": "Online CVs\n\nGoogle Scholar\nLinkedIn\nLattes Platform – Brazilian government’s research registry"
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "",
    "text": "Classifying Machinery Faults with PyTorch: A Complete End-to-End Workflow\nPredictive maintenance and fault detection are critical in industrial applications to avoid costly downtime and ensure worker safety. This blog post walks you through an entire pipeline for classifying mechanical faults using multivariate time-series data from the MAFAULDA dataset. We’ll cover every step in the workflow:\nThroughout this tutorial, you’ll see real code snippets (in Python) drawn from the repository structure below:\nLet’s dive in!"
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#exploratory-data-analysis-eda",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#exploratory-data-analysis-eda",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "1. Exploratory Data Analysis (EDA)",
    "text": "1. Exploratory Data Analysis (EDA)\n\nUnderstanding the Data\nWe are dealing with vibration and microphone signals recorded from a motor simulator under two conditions:\n1. Normal\n2. Imbalance (faulty)\nEach recording is ~5 seconds at a 50 kHz sampling rate, resulting in 250,000 samples per sensor. The dataset includes eight features:\n\ntachometer\nunderhang_axial\nunderhang_radiale\nunderhang_tangential\noverhang_axial\noverhang_radiale\noverhang_tangential\nmicrophone\n\nThe first step is to get a sense of data distributions, missing values, basic statistics, etc. In EDA.ipynb, you’ll see code like:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ydata_profiling import ProfileReport\n\n# Loading a CSV file for normal condition\ncol_names = [\n    'tachometer', 'underhang_axial', 'underhang_radiale', 'underhang_tangential',\n    'overhang_axial', 'overhang_radiale', 'overhang_tangential', 'microphone'\n]\nnormal_df = pd.read_csv(\"path/to/some_normal.csv\", names=col_names)\n\n# Generate HTML report (ydata-profiling)\nprofile = ProfileReport(normal_df, title=\"Normal Data\")\nprofile.to_file(\"normal_data_report.html\")\nThe generated report normal_data_report.html quickly reveals distributions and correlations. This helps us spot which features might be redundant or highly correlated. For instance, you might notice that microphone and underhang_radiale have high correlation, which could inform feature selection later.\n\n\nVisualizing Time-Series\nTo visualize how signal data changes over time, we can create quick plots of 50,000 samples (1 second snippet) from each class:\ndef plot_timeseries(df, columns, n_samples=50000):\n    plt.figure(figsize=(12, len(columns)))\n    for i, col in enumerate(columns, 1):\n        plt.subplot(len(columns), 1, i)\n        plt.plot(df[col].values[:n_samples])\n        plt.title(f\"Time Series of {col}\")\n    plt.tight_layout()\n    plt.show()\n\nplot_timeseries(normal_df, columns=normal_df.columns)\n\n\n\nts-normal\n\n\nObserving a few thousand samples reveals distinct behaviors between normal and imbalance signals—often manifested in amplitude changes or new frequency components."
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#feature-engineering",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#feature-engineering",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "2. Feature Engineering",
    "text": "2. Feature Engineering\nHigh-frequency time-series data can be noisy and large, so feature engineering becomes crucial. Two primary transformations were used in FeatureEngineering.ipynb and ultimately in the main pipeline:\n\nDownsampling: Consolidates raw data by averaging every b samples. In the repository, b=2500 was used, reducing 250,000 samples per file to just 100 samples.\n\nRolling Mean: Applies a moving average (with a given window size) to smooth abrupt fluctuations and incorporate temporal context into each feature.\n\nIn FeatureEngineering.ipynb, you’ll see:\ndef downSampler(data, b):\n    \"\"\"\n    Downsamples the given DataFrame by averaging every 'b' rows.\n    \"\"\"\n    return data.groupby(data.index // b).mean().reset_index(drop=True)\n\n\ndef rolling_mean_data(df, window_size=100, columns=None):\n    \"\"\"\n    Applies a rolling mean transformation to specified columns.\n    \"\"\"\n    if columns is None:\n        columns = df.select_dtypes(include=[np.number]).columns\n\n    df_copy = df.copy()\n    df_copy[columns] = df_copy[columns].rolling(window=window_size, min_periods=1).mean()\n    return df_copy\n\n# Usage:\nnormal_df = downSampler(normal_df, 2500)\nnormal_df = rolling_mean_data(normal_df, window_size=100)\n\n\n\nts-normal\n\n\n\nt-SNE Visualization for Feature Separability\nAfter feature transformations, we often test whether the classes (normal vs. imbalance) are more distinguishable in a compressed 2D space using t-SNE. If t-SNE shows two visually separate clusters, it’s a good sign that your features are linearly or nonlinearly separable:\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n\ndef plot_tsne(df, label_column='label'):\n    features = df.select_dtypes(include=[np.number]).drop(columns=[label_column], errors='ignore')\n    features_scaled = StandardScaler().fit_transform(features)\n    \n    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n    tsne_results = tsne.fit_transform(features_scaled)\n\n    # Plot\n    plt.figure(figsize=(8, 6))\n    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=df[label_column], cmap=\"viridis\", alpha=0.7)\n    plt.title(\"t-SNE Visualization\")\n    plt.show()\n\n\n\nts-normal"
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#building-training-a-multi-layer-perceptron-mlp-in-pytorch",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#building-training-a-multi-layer-perceptron-mlp-in-pytorch",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "3. Building & Training a Multi-Layer Perceptron (MLP) in PyTorch",
    "text": "3. Building & Training a Multi-Layer Perceptron (MLP) in PyTorch\nWhile more sophisticated sequence models (e.g., LSTM, 1D CNNs) can be used for time-series data, a Multi-Layer Perceptron often suffices when you’ve already introduced basic temporal smoothing in your features.\n\n3.1 The Dataset & DataLoader\nIn PyTorch, we create a custom Dataset class to handle how features and labels are fed to the model:\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass MachineryDataset(Dataset):\n    def __init__(self, data, label_column='label'):\n        self.labels = data[label_column].values\n        self.features = data.drop(columns=[label_column, 'time'], errors='ignore').values.astype('float32')\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        x = self.features[idx]\n        y = self.labels[idx]\n        return x, y\n\n__getitem__: Returns a single sample (features, label).\n__len__: Provides the total length of the dataset.\n\nWe also build a DataLoader object that batches the data and shuffles it during training:\ntrain_dataset = MachineryDataset(all_data, label_column='label')\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n\n3.2 MLP Model Architecture\nA simple feed-forward neural network can be built using fully connected layers (nn.Linear):\nimport torch.nn as nn\n\nclass TimeSeriesMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim=64, num_classes=2):\n        super(TimeSeriesMLP, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_classes)\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\nWe accept input_dim as the number of features (after downsampling & rolling mean).\n\nWe use two hidden layers with ReLU activation.\n\nThe final layer maps to num_classes=2 (binary classification: normal vs. imbalance).\n\n\n\n3.3 Training Loop\nPyTorch training typically involves: 1. Forward Pass: Pass input through the model to get predictions.\n2. Loss Calculation: Compute the discrepancy between predictions and labels (e.g., CrossEntropyLoss for classification).\n3. Backward Pass & Optimization: Update weights using gradient descent (e.g., optimizer.step()).\nIn MLP_Training.ipynb, a training loop might look like:\ndef train_model(model, train_loader, criterion, optimizer, epochs=10, device='cpu'):\n    model.to(device)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for X, y in train_loader:\n            X, y = X.to(device), y.long().to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(X)\n            loss = criterion(outputs, y)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() * X.size(0)\n            \n            # Calculate training accuracy\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == y).sum().item()\n            total += y.size(0)\n        \n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = correct / total\n        \n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}\")\n\nCrossEntropyLoss() is standard for multi-class/binary classification.\n\nWe track both loss and accuracy each epoch."
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#putting-it-all-together-in-main.py",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#putting-it-all-together-in-main.py",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "4. Putting It All Together in main.py",
    "text": "4. Putting It All Together in main.py\nFinally, in src/main.py we orchestrate the entire workflow: 1. Data Ingestion & Labeling\n2. Feature Engineering (Downsampling, Rolling Mean, StandardScaler)\n3. Splitting (Time-series split into train/val/test sets)\n4. MLP Training\n5. Evaluation: Accuracy, F1, Precision, Recall, AUC-ROC\nBelow is a condensed snippet showing the pipeline’s main logic:\n# Load normal and imbalance data\nnormal_dfs = load_filtered_dfs(data_path, \"normal\")\nimbalance_dfs = load_filtered_dfs(data_path, \"imbalance-6g\")\n\n# Apply augmentation (downsampling + rolling) to each DF, then concat\nnormal_df = pd.concat([augment_features(df) for df in normal_dfs], ignore_index=True)\nimbalance_df = pd.concat([augment_features(df) for df in imbalance_dfs], ignore_index=True)\n\n# Label the data\nnormal_df[\"label\"] = 0\nimbalance_df[\"label\"] = 1\n\nall_data = pd.concat([normal_df, imbalance_df], ignore_index=True)\n\n# Show correlation matrix\nsave_correlation_matrix(all_data)\n\n# t-SNE visualization\nplot_tsne(all_data, label_column='label', output_file=\"../figures/tsne_visualization.png\")\n\n# Time-series split (train/val/test)\ntrain_data, val_data, test_data = time_series_split(all_data)\n\n# Normalize features\nscaler = StandardScaler()\ntrain_data.iloc[:, :-1] = scaler.fit_transform(train_data.iloc[:, :-1])\nval_data.iloc[:, :-1] = scaler.transform(val_data.iloc[:, :-1])\ntest_data.iloc[:, :-1] = scaler.transform(test_data.iloc[:, :-1])\n\n# Datasets & Loaders\ntrain_dataset = MachineryDataset(train_data)\nval_dataset = MachineryDataset(val_data)\ntest_dataset = MachineryDataset(test_data)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Initialize and train MLP\nmodel = TimeSeriesMLP(\n    input_dim=train_dataset.features.shape[1],\n    hidden_dim=3,\n    n_layers=2\n)\nhistory = train_model(model, train_loader, val_loader)  # default epochs=50\n\n# Evaluate\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\ntest_metrics = evaluate_model(model, test_loader)\nplot_evaluation_results(test_metrics, output_file=\"../figures/evaluation_plot.png\")\n\nEvaluation & Metrics\nAfter training, we apply the model to the test set. For binary classification, we typically measure:\n\nAccuracy: Ratio of correct predictions over total.\n\nF1-score: Harmonic mean of precision & recall.\n\nPrecision: Among predicted positives, how many are truly positive?\n\nRecall: Among all actual positives, how many did we predict correctly?\n\nAUC-ROC: Area under the Receiver Operating Characteristic curve (only if you have more than one class present).\n\nHere’s how it’s done in main.py:\ndef evaluate_model(model, test_loader, threshold=0.5):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            outputs = model(inputs).squeeze()\n            preds = (outputs &gt; threshold).float()\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, zero_division=0)\n    precision = precision_score(all_labels, all_preds, zero_division=0)\n    recall = recall_score(all_labels, all_preds, zero_division=0)\n    auc_roc = roc_auc_score(all_labels, all_preds) if len(np.unique(all_labels)) &gt; 1 else None\n\n    metrics = {\n        \"Accuracy\": accuracy,\n        \"F1-score\": f1,\n        \"Precision\": precision,\n        \"Recall\": recall,\n        \"AUC-ROC\": auc_roc\n    }\n    \n    for metric, value in metrics.items():\n        if value is not None:\n            print(f\"{metric}: {value:.4f}\")\n\n    return metrics\n\n\n\nts-normal\n\n\nExcellent performance, which suggests that even a relatively straightforward MLP can separate normal vs. imbalance classes well, thanks to feature engineering (downsampling + rolling mean)."
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#next-steps-enhancements",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#next-steps-enhancements",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "5. Next Steps & Enhancements",
    "text": "5. Next Steps & Enhancements\n\nHyperparameter Optimization\n\nTest different hidden layer sizes, dropout probabilities, and learning rates.\n\nConsider GridSearch or Bayesian Optimization for an automated approach.\n\nInclude More Fault Conditions\n\nThe MAFAULDA dataset has multiple fault types (unbalance, misalignment, bearing faults). Extending beyond just normal vs. imbalance classification can add realism.\n\nSequence Models\n\nFor a deeper time-series approach, experiment with CNNs (1D Convolutions) or LSTM architectures. Those are better at capturing sequential dependencies without relying only on rolling averages.\n\nReal-Time Inference\n\nDeploy the trained model in a streaming or edge environment for real-time fault detection in industrial settings."
  },
  {
    "objectID": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#conclusion",
    "href": "posts/classify-time-series-faults-pytorch/classify-time-series-faults-pytorch.html#conclusion",
    "title": "Classify Time-Series Faults with Pytorch",
    "section": "Conclusion",
    "text": "Conclusion\nWe’ve walked through a complete pipeline for classifying mechanical faults using time-series data. The key lessons include:\n\nEDA is indispensable for quickly assessing data quality and distributions.\n\nFeature Engineering (downsampling, rolling means) can drastically reduce data size and highlight meaningful trends.\n\nEven a basic MLP can achieve high accuracy if the features reflect the underlying process well.\n\nEvaluation metrics (Accuracy, F1, Precision, Recall, AUC-ROC) are critical to understand true performance.\n\nIf you’re looking to adapt this pipeline to your own fault classification tasks—whether it’s rotating machinery, bearings, or other mechanical equipment—these concepts should be straightforward to customize. Feel free to explore the MAFAULDA dataset for your own experiments or extend it with advanced deep learning architectures.\nThanks for reading, and happy fault detecting!\n\n\nAdditional Resources\n\nPyTorch Documentation\n\nTime-series EDA Techniques\n\nAdvanced Feature Engineering for Vibration Data"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Data Scientist with a strong background in machine learning, optimization, and MLOps, focused on developing scalable AI solutions and deploying models into production. I hold a Bachelor’s in Electronic Engineering and a Master’s in Information Systems, with international academic experience across Japan, the United Kingdom, and Canada. This diverse background has equipped me with strong analytical skills, the ability to quickly understand and implement state-of-the-art research, and a scientific approach to solving complex industry challenges.\nIn my industry experience, I have worked extensively with machine learning and deep learning frameworks to train and fine-tune models for predictive modeling, anomaly detection, and computer vision. My expertise in MLOps enables me to design and implement end-to-end pipelines that ensure reproducibility and scalability. Leveraging tools like MLflow, Docker, and cloud platforms like AWS and DigitalOcean, I have successfully deployed machine learning models and integrated them into production via API services.\nWith a strong statistical analysis and optimization foundation, I am particularly interested in Generative AI and LLMs, actively exploring their potential in real-world applications. I am eager to contribute to cutting-edge AI solutions and look forward to collaborating with professionals and organizations driving AI innovation in production environments."
  },
  {
    "objectID": "about.html#technical-expertise",
    "href": "about.html#technical-expertise",
    "title": "About",
    "section": "Technical Expertise",
    "text": "Technical Expertise\n\nOperating Systems:\n Linux (Fedora, Ubuntu),  Windows.\n\n\n\nProgramming Languages:\n Python,  C,  C++,  Bash (Unix Shell).\n\n\n\nMachine Learning Frameworks:\n PyTorch,  scikit-learn,  XGBoost, FastAI.\n\n\n\nMLOps & Deployment Tools:\n Docker,  MLflow,  Hydra,  Git,  CI/CD (GitLab, GitHub Actions),  FastAPI.\n\n\n\nData Engineering & Pipelines:\n NumPy,  Pandas, Dask,  PostgreSQL, Redshift, InfluxDB.\n\n\n\nCloud Platforms:\n AWS,  DigitalOcean.\n\n\n\nOptimization:\nBayesian Optimization, Multi-objective Evolutionary Algorithms."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "In the dynamic world of short-term property rentals, accurately predicting rental prices is crucial for maximizing occupancy and revenue. To address this challenge, I developed a comprehensive, reproducible Machine Learning (ML) pipeline tailored for short-term rental price prediction in New York City (NYC) based on Airbnb data. We walk through data collection, cleaning, validation, splitting, training a random forest model, testing the model, and logging every artifact and result in a reproducible manner. This blog post focus on the usage of MLflow, Weights & Biases (W&B), and Hydra for orchestration, tracking, and hyperparameter optimization, as these are taught in the Udacity MLOps Nanodegree program.\n\n\nShort-term rental platforms like Airbnb collect vast amounts of data from hosts and guests. A host often asks: What price should I set for my listing to optimize occupancy and revenue? This project aims to solve that by building an end-to-end ML pipeline to predict a short-term rental’s price given various features such as location, room type, and number of reviews.\n\nFocus: Clean, reproducible, and easily re-runnable pipeline.\nTools: MLflow for orchestration, Hydra for configuration, Weights & Biases to log artifacts and metrics, and scikit-learn for model training.\nAdditional: ydata-profiling for EDA, custom data checks to prevent “data drift,” and a templated approach to add new steps using a cookiecutter MLflow step.\n\nThe entire pipeline is tracked under a W&B project to enable experiment tracking and artifact storage. Here is the link for the completed project in W&B.\n\n\n\nThe project is organized as follows:\n.\n├── components/                  # Reusable pipeline components\n│   ├── get_data/                # Data ingestion module\n│   ├── test_regression_model/   # Model testing module\n│   └── train_val_test_split/    # Data splitting module\n├── src/                         # Source code for main pipeline steps\n│   ├── basic_cleaning/          # Data cleaning scripts\n│   ├── data_check/              # Data validation scripts\n│   ├── eda/                     # Exploratory Data Analysis scripts\n│   └── train_random_forest/     # Model training scripts\n├── images/                      # Visual assets for documentation\n├── .github/workflows/           # GitHub Actions for CI/CD\n├── cookie-mlflow-step/          # Template for creating new pipeline steps\n├── conda.yml                    # Conda environment configuration\n├── config.yaml                  # Pipeline configuration settings\n├── environment.yml              # Alternative environment configuration\n├── main.py                      # Main script to run the pipeline\n├── MLproject                    # MLflow project specification\n└── README.md                    # Project documentation\nKey folders:\n\ncomponents/: Reusable MLflow components for tasks like data downloading, data splitting, and model testing.\n\nsrc/: Specific pipeline steps, including EDA (eda), data cleaning (basic_cleaning), data checks (data_check), and training a random forest (train_random_forest).\n\ncookie-mlflow-step/: A cookiecutter template that quickly scaffolds new MLflow pipeline steps.\n\nmain.py: The orchestrator that references each step through Hydra configuration.\n\nMLproject: Defines how MLflow runs the pipeline, specifying entry points and environment details.\n\n\n\n\n\n\nThis module handles the retrieval of raw data, ensuring it’s stored in a structured format suitable for downstream processing. It interfaces with data sources, downloads datasets, and logs them as artifacts for version control.\n\n\n\nData cleaning involves:\n\nRemoving duplicates and irrelevant entries.\nHandling missing values through imputation or removal.\nCorrecting data types and formatting issues.\nAddressing outliers to prevent skewed model training.\n\n\n\n\nEDA provides insights into the dataset through:\n\nStatistical summaries of features.\nVisualizations to identify patterns and correlations.\nDetection of anomalies or unexpected distributions.\n\n\n\n\nBefore model training, data validation checks are performed to ensure:\n\nConsistency in data formats and ranges.\nIntegrity constraints are maintained.\nAlignment with expected distributions to prevent data drift.\n\n\n\n\nThe dataset is partitioned into:\n\nTraining Set: For model learning.\nValidation Set: For hyperparameter tuning and model selection.\nTest Set: For final evaluation of model performance.\n\n\n\n\nUtilizing the Random Forest algorithm, this step involves:\n\nTraining the model on the prepared dataset.\nLogging training parameters and metrics.\nSaving the trained model artifact for evaluation and deployment.\n\n\n\n\nThe trained model undergoes rigorous evaluation to assess:\n\nPredictive accuracy on unseen data.\nGeneralization capabilities.\nPotential overfitting or underfitting issues.\n\n\n\n\n\n\nWe rely on two environment files:\n\nenvironment.yml: Sets up the main environment (nyc_airbnb_dev) with Python 3.10, Hydra, Jupyter, and crucial Python packages (MLflow, W&B, ydata-profiling, etc.).\n\nconda.yml (in various subfolders): Each step can be run in a mini environment with the dependencies it needs.\n\nTo install:\nconda env create -f environment.yml\nconda activate nyc_airbnb_dev\nwandb login [your_API_key]\nAfter this, you can run:\nmlflow run . \nMLflow will pick up the MLproject file in the root directory and execute main.py.\n\n\n\n\n\n\nThe code for downloading data is stored in components/get_data. We keep a couple of sample CSVs in data/ that stand in for a real-world dataset. This step simply logs an artifact to W&B.\nCode Snippet from components/get_data/run.py\ndef go(args):\n    run = wandb.init(job_type=\"download_file\")\n    run.config.update(args)\n\n    log_artifact(\n        args.artifact_name,\n        args.artifact_type,\n        args.artifact_description,\n        os.path.join(\"data\", args.sample),\n        run,\n    )\nTo run just the download step:\nmlflow run . -P steps=download\n\n\n\nWe have an EDA folder (src/eda). It contains a EDA.ipynb notebook, which uses ydata-profiling for generating a quick profile report of the dataset. The snippet below shows how we generate an HTML report and log it to W&B.\nCode Snippet from src/eda/EDA.ipynb\nimport ydata_profiling\nprofile = ydata_profiling.ProfileReport(df)\nprofile.to_file(\"profile-report.html\")\nartifact = wandb.Artifact(\n    name=\"profile-report.html\", \n    type=\"analysis\", \n    description=\"Report from ydata-profiling\"\n)\nartifact.add_file(\"profile-report.html\")\nrun.log_artifact(artifact)\nWe also create various plots (like price distribution) and attempt to remove obvious outliers (price &lt;10 or &gt;350) to get a more reasonable dataset.\nKey Observations:\n- Some columns like last_review and reviews_per_month can have many missing values.\n- The distribution of price is highly skewed.\n- We remove out-of-bound lat/long values that are not within the known NYC boundaries.\n\n\n\nA dedicated step in src/basic_cleaning/ cleans the data after EDA reveals certain constraints.\nWe used Cookiecutter to generate the skeleton for this step and then filled in the logic. The run.py file:\nmlflow run . -P steps=basic_cleaning\nCore snippet from src/basic_cleaning/run.py:\ndf = df.drop_duplicates().reset_index(drop=True)\nidx = df['price'].between(args.min_price, args.max_price)\ndf = df[idx].copy()\nidx = df['longitude'].between(-74.25, -73.50) & df['latitude'].between(40.5, 41.2)\ndf = df[idx].copy()\n\n# Log the cleaned CSV as W&B artifact\nartifact = wandb.Artifact(\n    name=args.output_artifact,\n    type=args.output_type,\n    description=args.output_description,\n)\nartifact.add_file(fp.name)\nrun.log_artifact(artifact)\nIt:\n1. Drops duplicates.\n2. Filters out out-of-range prices.\n3. Ensures lat/long within valid NYC boundary.\n4. Logs the cleaned dataset to W&B.\n\n\n\nWe follow the concept of Data Testing to guard against “data pipeline rot.” The step is in src/data_check/. It uses pytest tests to verify that the cleaned data:\n\nHas valid column names.\nFalls within expected lat/long boundaries.\nContains only known neighborhoods.\nDistributes similarly to a reference dataset (via KL divergence).\nRespects a minimum and maximum price range.\n\nA snippet of the test suite from src/data_check/test_data.py:\ndef test_similar_neigh_distrib(data: pd.DataFrame, ref_data: pd.DataFrame, kl_threshold: float):\n    dist1 = data['neighbourhood_group'].value_counts().sort_index()\n    dist2 = ref_data['neighbourhood_group'].value_counts().sort_index()\n    assert scipy.stats.entropy(dist1, dist2, base=2) &lt; kl_threshold\nTo run:\nmlflow run . -P steps=data_check\nAny mismatches or anomalies raise an exception that stops the pipeline, keeping you from training on “bad” data.\n\n\n\nWe then split our dataset into training, validation, and test sets (the last set is strictly for final model testing). The relevant code is in components/train_val_test_split/.\nCode Snippet from components/train_val_test_split/run.py\ntrainval, test = train_test_split(\n    df,\n    test_size=args.test_size,\n    random_state=args.random_seed,\n    stratify=df[args.stratify_by] if args.stratify_by != 'none' else None,\n)\nBoth the training/validation split (“trainval_data.csv”) and the test split (“test_data.csv”) are then logged to W&B.\n\n\n\nWith a clean train-validation dataset, we build a random forest pipeline in src/train_random_forest/run.py. This step is heavily reliant on Hydra for configuration. We define parameters like max_depth, n_estimators, etc., in config.yaml.\nHere’s a highlight from run.py:\ndef get_inference_pipeline(rf_config, max_tfidf_features):\n    ordinal_categorical = [\"room_type\"]\n    non_ordinal_categorical = [\"neighbourhood_group\"]\n    \n    ordinal_categorical_preproc = OrdinalEncoder()\n    non_ordinal_categorical_preproc = Pipeline([\n        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"encode\", OneHotEncoder())\n    ])\n    ...\n    random_forest = RandomForestRegressor(**rf_config)\n    sk_pipe = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"random_forest\", random_forest),\n    ])\n    return sk_pipe, processed_features\nIn this pipeline, we handle:\n- Categorical columns (OrdinalEncoder or OneHotEncoder).\n- Numerical columns (imputation for missing values).\n- NLP on the name field using a TF-IDF vectorizer.\nWe train and evaluate on a validation set, logging all metrics (MAE, R^2) to W&B. Finally, the pipeline (preprocessing + model) is saved to MLflow format and uploaded to W&B as an artifact:\nmlflow.sklearn.save_model(\n    sk_pipe,\n    export_path,\n    serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE,\n    signature=sig,\n    input_example=X_val[processed_features].iloc[:2]\n)\nBy storing the entire pipeline, we can apply transformations consistently at inference time.\nTo run the model training step:\nmlflow run . -P steps=train_random_forest\n\n\n\nLastly, we evaluate our finalized model against the test set. The code is in components/test_regression_model/run.py. It:\n\nDownloads the prod model artifact from W&B.\n\nLoads the test dataset.\n\nGenerates predictions and calculates R^2 and MAE.\n\nLogs test performance to W&B.\n\ndef go(args):\n    sk_pipe = mlflow.sklearn.load_model(model_local_path)\n    y_pred = sk_pipe.predict(X_test)\n    r_squared = sk_pipe.score(X_test, y_test)\n    mae = mean_absolute_error(y_test, y_pred)\n\n    run.summary['r2'] = r_squared\n    run.summary['mae'] = mae\nYou only run this step after a model has been tagged for production, ensuring it has proven to be stable in dev/validation.\n\n\n\n\n\nThe main.py script orchestrates the entire flow. It reads config.yaml via Hydra and decides which steps to run based on a comma-separated list. A minimal snippet:\n_steps = [\n    \"download\",\n    \"basic_cleaning\",\n    \"data_check\",\n    \"data_split\",\n    \"train_random_forest\",\n    # \"test_regression_model\" is triggered separately\n]\n\n@hydra.main(config_name='config')\ndef go(config: DictConfig):\n\n    steps_par = config['main']['steps']\n    active_steps = steps_par.split(\",\") if steps_par != \"all\" else _steps\n\n    if \"download\" in active_steps:\n        mlflow.run(\n            f\"{config['main']['components_repository']}/get_data\",\n            ...\n        )\n\n    if \"basic_cleaning\" in active_steps:\n        mlflow.run(\n            os.path.join(hydra.utils.get_original_cwd(), \"src\", \"basic_cleaning\"),\n            ...\n        )\n\n    ...\nTo execute the entire pipeline:\nmlflow run . \nOr pick and choose steps:\nmlflow run . -P steps=download,basic_cleaning,data_check\n\n\n\n\nOne of the repository’s highlights is the cookie-mlflow-step folder, which speeds up adding new steps to the pipeline:\ncookiecutter cookie-mlflow-step -o src\nIt scaffolds a new directory structure containing an MLproject, conda.yml, and a run.py pre-populated with arguments. This is helpful for consistent, standardized pipeline steps where each step is an MLflow project.\n\n\n\n\n\nCI/CD: The .github/workflows/manual.yml shows how a manual GH Action can create Jira tickets for new Pull Requests.\n\nRelease: Tagging your repo with a version number (e.g., 1.0.0) allows you to run the pipeline from a specific commit. For instance:\nmlflow run https://github.com/yurimarca/build-ml-pipeline-for-short-term-rental-prices.git \\\n  -v 1.0.0 \\\n  -P hydra_options=\"etl.sample='sample2.csv'\"\nArtifact Logging: W&B captures every CSV and model artifact, so future steps or collaborators can trace lineage and retrieve them easily.\n\n\n\n\n\nThis repository combines Hydra for flexible configuration, MLflow for pipeline orchestration, and Weights & Biases for experiment tracking to create a fully reproducible short-term rental price prediction pipeline in NYC.\nKey Takeaways:\n\nData integrity: By integrating data validation tests, the pipeline can fail early if data is incorrect.\n\nReproducible training: Using Hydra, MLflow, and environment files ensures consistent environments and parameter definitions.\n\nFull pipeline tracking: From EDA to final test, each artifact is logged to W&B, making it easy to revert or compare different runs.\n\nExtensibility: The cookiecutter approach helps you quickly add new pipeline steps or replicate the same pipeline structure in other projects.\n\nFeel free to explore the code base, and don’t hesitate to experiment by customizing steps or hyperparameters. By following this pipeline, you can keep your machine learning workflow tidy, versioned, and production-ready.\nFor a detailed walkthrough and access to the codebase, visit the GitHub repository.\nNote: This project was developed as part of the Udacity MLOps Nanodegree program."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#project-overview",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#project-overview",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "Short-term rental platforms like Airbnb collect vast amounts of data from hosts and guests. A host often asks: What price should I set for my listing to optimize occupancy and revenue? This project aims to solve that by building an end-to-end ML pipeline to predict a short-term rental’s price given various features such as location, room type, and number of reviews.\n\nFocus: Clean, reproducible, and easily re-runnable pipeline.\nTools: MLflow for orchestration, Hydra for configuration, Weights & Biases to log artifacts and metrics, and scikit-learn for model training.\nAdditional: ydata-profiling for EDA, custom data checks to prevent “data drift,” and a templated approach to add new steps using a cookiecutter MLflow step.\n\nThe entire pipeline is tracked under a W&B project to enable experiment tracking and artifact storage. Here is the link for the completed project in W&B."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#repository-structure",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#repository-structure",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "The project is organized as follows:\n.\n├── components/                  # Reusable pipeline components\n│   ├── get_data/                # Data ingestion module\n│   ├── test_regression_model/   # Model testing module\n│   └── train_val_test_split/    # Data splitting module\n├── src/                         # Source code for main pipeline steps\n│   ├── basic_cleaning/          # Data cleaning scripts\n│   ├── data_check/              # Data validation scripts\n│   ├── eda/                     # Exploratory Data Analysis scripts\n│   └── train_random_forest/     # Model training scripts\n├── images/                      # Visual assets for documentation\n├── .github/workflows/           # GitHub Actions for CI/CD\n├── cookie-mlflow-step/          # Template for creating new pipeline steps\n├── conda.yml                    # Conda environment configuration\n├── config.yaml                  # Pipeline configuration settings\n├── environment.yml              # Alternative environment configuration\n├── main.py                      # Main script to run the pipeline\n├── MLproject                    # MLflow project specification\n└── README.md                    # Project documentation\nKey folders:\n\ncomponents/: Reusable MLflow components for tasks like data downloading, data splitting, and model testing.\n\nsrc/: Specific pipeline steps, including EDA (eda), data cleaning (basic_cleaning), data checks (data_check), and training a random forest (train_random_forest).\n\ncookie-mlflow-step/: A cookiecutter template that quickly scaffolds new MLflow pipeline steps.\n\nmain.py: The orchestrator that references each step through Hydra configuration.\n\nMLproject: Defines how MLflow runs the pipeline, specifying entry points and environment details."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#pipeline-components",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#pipeline-components",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "This module handles the retrieval of raw data, ensuring it’s stored in a structured format suitable for downstream processing. It interfaces with data sources, downloads datasets, and logs them as artifacts for version control.\n\n\n\nData cleaning involves:\n\nRemoving duplicates and irrelevant entries.\nHandling missing values through imputation or removal.\nCorrecting data types and formatting issues.\nAddressing outliers to prevent skewed model training.\n\n\n\n\nEDA provides insights into the dataset through:\n\nStatistical summaries of features.\nVisualizations to identify patterns and correlations.\nDetection of anomalies or unexpected distributions.\n\n\n\n\nBefore model training, data validation checks are performed to ensure:\n\nConsistency in data formats and ranges.\nIntegrity constraints are maintained.\nAlignment with expected distributions to prevent data drift.\n\n\n\n\nThe dataset is partitioned into:\n\nTraining Set: For model learning.\nValidation Set: For hyperparameter tuning and model selection.\nTest Set: For final evaluation of model performance.\n\n\n\n\nUtilizing the Random Forest algorithm, this step involves:\n\nTraining the model on the prepared dataset.\nLogging training parameters and metrics.\nSaving the trained model artifact for evaluation and deployment.\n\n\n\n\nThe trained model undergoes rigorous evaluation to assess:\n\nPredictive accuracy on unseen data.\nGeneralization capabilities.\nPotential overfitting or underfitting issues."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#setting-up-the-environment",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#setting-up-the-environment",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "We rely on two environment files:\n\nenvironment.yml: Sets up the main environment (nyc_airbnb_dev) with Python 3.10, Hydra, Jupyter, and crucial Python packages (MLflow, W&B, ydata-profiling, etc.).\n\nconda.yml (in various subfolders): Each step can be run in a mini environment with the dependencies it needs.\n\nTo install:\nconda env create -f environment.yml\nconda activate nyc_airbnb_dev\nwandb login [your_API_key]\nAfter this, you can run:\nmlflow run . \nMLflow will pick up the MLproject file in the root directory and execute main.py."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#pipeline-steps",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#pipeline-steps",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "The code for downloading data is stored in components/get_data. We keep a couple of sample CSVs in data/ that stand in for a real-world dataset. This step simply logs an artifact to W&B.\nCode Snippet from components/get_data/run.py\ndef go(args):\n    run = wandb.init(job_type=\"download_file\")\n    run.config.update(args)\n\n    log_artifact(\n        args.artifact_name,\n        args.artifact_type,\n        args.artifact_description,\n        os.path.join(\"data\", args.sample),\n        run,\n    )\nTo run just the download step:\nmlflow run . -P steps=download\n\n\n\nWe have an EDA folder (src/eda). It contains a EDA.ipynb notebook, which uses ydata-profiling for generating a quick profile report of the dataset. The snippet below shows how we generate an HTML report and log it to W&B.\nCode Snippet from src/eda/EDA.ipynb\nimport ydata_profiling\nprofile = ydata_profiling.ProfileReport(df)\nprofile.to_file(\"profile-report.html\")\nartifact = wandb.Artifact(\n    name=\"profile-report.html\", \n    type=\"analysis\", \n    description=\"Report from ydata-profiling\"\n)\nartifact.add_file(\"profile-report.html\")\nrun.log_artifact(artifact)\nWe also create various plots (like price distribution) and attempt to remove obvious outliers (price &lt;10 or &gt;350) to get a more reasonable dataset.\nKey Observations:\n- Some columns like last_review and reviews_per_month can have many missing values.\n- The distribution of price is highly skewed.\n- We remove out-of-bound lat/long values that are not within the known NYC boundaries.\n\n\n\nA dedicated step in src/basic_cleaning/ cleans the data after EDA reveals certain constraints.\nWe used Cookiecutter to generate the skeleton for this step and then filled in the logic. The run.py file:\nmlflow run . -P steps=basic_cleaning\nCore snippet from src/basic_cleaning/run.py:\ndf = df.drop_duplicates().reset_index(drop=True)\nidx = df['price'].between(args.min_price, args.max_price)\ndf = df[idx].copy()\nidx = df['longitude'].between(-74.25, -73.50) & df['latitude'].between(40.5, 41.2)\ndf = df[idx].copy()\n\n# Log the cleaned CSV as W&B artifact\nartifact = wandb.Artifact(\n    name=args.output_artifact,\n    type=args.output_type,\n    description=args.output_description,\n)\nartifact.add_file(fp.name)\nrun.log_artifact(artifact)\nIt:\n1. Drops duplicates.\n2. Filters out out-of-range prices.\n3. Ensures lat/long within valid NYC boundary.\n4. Logs the cleaned dataset to W&B.\n\n\n\nWe follow the concept of Data Testing to guard against “data pipeline rot.” The step is in src/data_check/. It uses pytest tests to verify that the cleaned data:\n\nHas valid column names.\nFalls within expected lat/long boundaries.\nContains only known neighborhoods.\nDistributes similarly to a reference dataset (via KL divergence).\nRespects a minimum and maximum price range.\n\nA snippet of the test suite from src/data_check/test_data.py:\ndef test_similar_neigh_distrib(data: pd.DataFrame, ref_data: pd.DataFrame, kl_threshold: float):\n    dist1 = data['neighbourhood_group'].value_counts().sort_index()\n    dist2 = ref_data['neighbourhood_group'].value_counts().sort_index()\n    assert scipy.stats.entropy(dist1, dist2, base=2) &lt; kl_threshold\nTo run:\nmlflow run . -P steps=data_check\nAny mismatches or anomalies raise an exception that stops the pipeline, keeping you from training on “bad” data.\n\n\n\nWe then split our dataset into training, validation, and test sets (the last set is strictly for final model testing). The relevant code is in components/train_val_test_split/.\nCode Snippet from components/train_val_test_split/run.py\ntrainval, test = train_test_split(\n    df,\n    test_size=args.test_size,\n    random_state=args.random_seed,\n    stratify=df[args.stratify_by] if args.stratify_by != 'none' else None,\n)\nBoth the training/validation split (“trainval_data.csv”) and the test split (“test_data.csv”) are then logged to W&B.\n\n\n\nWith a clean train-validation dataset, we build a random forest pipeline in src/train_random_forest/run.py. This step is heavily reliant on Hydra for configuration. We define parameters like max_depth, n_estimators, etc., in config.yaml.\nHere’s a highlight from run.py:\ndef get_inference_pipeline(rf_config, max_tfidf_features):\n    ordinal_categorical = [\"room_type\"]\n    non_ordinal_categorical = [\"neighbourhood_group\"]\n    \n    ordinal_categorical_preproc = OrdinalEncoder()\n    non_ordinal_categorical_preproc = Pipeline([\n        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"encode\", OneHotEncoder())\n    ])\n    ...\n    random_forest = RandomForestRegressor(**rf_config)\n    sk_pipe = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"random_forest\", random_forest),\n    ])\n    return sk_pipe, processed_features\nIn this pipeline, we handle:\n- Categorical columns (OrdinalEncoder or OneHotEncoder).\n- Numerical columns (imputation for missing values).\n- NLP on the name field using a TF-IDF vectorizer.\nWe train and evaluate on a validation set, logging all metrics (MAE, R^2) to W&B. Finally, the pipeline (preprocessing + model) is saved to MLflow format and uploaded to W&B as an artifact:\nmlflow.sklearn.save_model(\n    sk_pipe,\n    export_path,\n    serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_CLOUDPICKLE,\n    signature=sig,\n    input_example=X_val[processed_features].iloc[:2]\n)\nBy storing the entire pipeline, we can apply transformations consistently at inference time.\nTo run the model training step:\nmlflow run . -P steps=train_random_forest\n\n\n\nLastly, we evaluate our finalized model against the test set. The code is in components/test_regression_model/run.py. It:\n\nDownloads the prod model artifact from W&B.\n\nLoads the test dataset.\n\nGenerates predictions and calculates R^2 and MAE.\n\nLogs test performance to W&B.\n\ndef go(args):\n    sk_pipe = mlflow.sklearn.load_model(model_local_path)\n    y_pred = sk_pipe.predict(X_test)\n    r_squared = sk_pipe.score(X_test, y_test)\n    mae = mean_absolute_error(y_test, y_pred)\n\n    run.summary['r2'] = r_squared\n    run.summary['mae'] = mae\nYou only run this step after a model has been tagged for production, ensuring it has proven to be stable in dev/validation."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#putting-it-all-together-main.py",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#putting-it-all-together-main.py",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "The main.py script orchestrates the entire flow. It reads config.yaml via Hydra and decides which steps to run based on a comma-separated list. A minimal snippet:\n_steps = [\n    \"download\",\n    \"basic_cleaning\",\n    \"data_check\",\n    \"data_split\",\n    \"train_random_forest\",\n    # \"test_regression_model\" is triggered separately\n]\n\n@hydra.main(config_name='config')\ndef go(config: DictConfig):\n\n    steps_par = config['main']['steps']\n    active_steps = steps_par.split(\",\") if steps_par != \"all\" else _steps\n\n    if \"download\" in active_steps:\n        mlflow.run(\n            f\"{config['main']['components_repository']}/get_data\",\n            ...\n        )\n\n    if \"basic_cleaning\" in active_steps:\n        mlflow.run(\n            os.path.join(hydra.utils.get_original_cwd(), \"src\", \"basic_cleaning\"),\n            ...\n        )\n\n    ...\nTo execute the entire pipeline:\nmlflow run . \nOr pick and choose steps:\nmlflow run . -P steps=download,basic_cleaning,data_check"
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#using-cookiecutter-for-quick-pipeline-steps",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#using-cookiecutter-for-quick-pipeline-steps",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "One of the repository’s highlights is the cookie-mlflow-step folder, which speeds up adding new steps to the pipeline:\ncookiecutter cookie-mlflow-step -o src\nIt scaffolds a new directory structure containing an MLproject, conda.yml, and a run.py pre-populated with arguments. This is helpful for consistent, standardized pipeline steps where each step is an MLflow project."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#deployment-and-workflow",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#deployment-and-workflow",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "CI/CD: The .github/workflows/manual.yml shows how a manual GH Action can create Jira tickets for new Pull Requests.\n\nRelease: Tagging your repo with a version number (e.g., 1.0.0) allows you to run the pipeline from a specific commit. For instance:\nmlflow run https://github.com/yurimarca/build-ml-pipeline-for-short-term-rental-prices.git \\\n  -v 1.0.0 \\\n  -P hydra_options=\"etl.sample='sample2.csv'\"\nArtifact Logging: W&B captures every CSV and model artifact, so future steps or collaborators can trace lineage and retrieve them easily."
  },
  {
    "objectID": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#conclusion",
    "href": "posts/end-to-end-ml-pipeline/end-to-end-ml-pipeline.html#conclusion",
    "title": "Building an End-to-End ML Pipeline",
    "section": "",
    "text": "This repository combines Hydra for flexible configuration, MLflow for pipeline orchestration, and Weights & Biases for experiment tracking to create a fully reproducible short-term rental price prediction pipeline in NYC.\nKey Takeaways:\n\nData integrity: By integrating data validation tests, the pipeline can fail early if data is incorrect.\n\nReproducible training: Using Hydra, MLflow, and environment files ensures consistent environments and parameter definitions.\n\nFull pipeline tracking: From EDA to final test, each artifact is logged to W&B, making it easy to revert or compare different runs.\n\nExtensibility: The cookiecutter approach helps you quickly add new pipeline steps or replicate the same pipeline structure in other projects.\n\nFeel free to explore the code base, and don’t hesitate to experiment by customizing steps or hyperparameters. By following this pipeline, you can keep your machine learning workflow tidy, versioned, and production-ready.\nFor a detailed walkthrough and access to the codebase, visit the GitHub repository.\nNote: This project was developed as part of the Udacity MLOps Nanodegree program."
  },
  {
    "objectID": "old_content/publication.html",
    "href": "old_content/publication.html",
    "title": "Yuri Marca’s Homepage",
    "section": "",
    "text": "Y. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. Approximating Pareto set topology by cubic interpolation on bi-objective problems. 10th International Conference on Evolutionary Multi-Criterion Optimization (EMO2019), Lecture Notes in Computer Science (LNCS), vol 11411, pp 386-398, East Lansing, Michigan, USA, 2019 DOI ★Best Student Paper Award★\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. NSGA-II with Spline Interpolation on Bi-objective Problems with Difficult Pareto Set Topology. JPNSEC 2018 Symposium on Evolutionary Computation, Fukuoka, 2018. ★Young Research Award★\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. MOEAs on Problems with Difficult Pareto Set Topologies. IEICE Shin-etsu Branch IEEE Session, Niigata University, 2018, p. 169. (presentation) ★Young Research Paper Award★\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. Pareto dominance-based MOEAs on problems with difficult pareto set topologies. In Proceedings of the Genetic and Evolutionary Computation Conference Companion (GECCO ’18). ACM, New York, NY, USA, 189-190. DOI.\nC. E. A. L. Rocha, Y. P. Marca, F. K. Schneider. Support Platform for Decision-Making in Research and Technological Development in Public Health. ESPACIOS (CARACAS), v. 39, p. 14-26, 2018. (link)"
  },
  {
    "objectID": "old_content/publication.html#publications",
    "href": "old_content/publication.html#publications",
    "title": "Yuri Marca’s Homepage",
    "section": "",
    "text": "Y. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. Approximating Pareto set topology by cubic interpolation on bi-objective problems. 10th International Conference on Evolutionary Multi-Criterion Optimization (EMO2019), Lecture Notes in Computer Science (LNCS), vol 11411, pp 386-398, East Lansing, Michigan, USA, 2019 DOI ★Best Student Paper Award★\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. NSGA-II with Spline Interpolation on Bi-objective Problems with Difficult Pareto Set Topology. JPNSEC 2018 Symposium on Evolutionary Computation, Fukuoka, 2018. ★Young Research Award★\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. MOEAs on Problems with Difficult Pareto Set Topologies. IEICE Shin-etsu Branch IEEE Session, Niigata University, 2018, p. 169. (presentation) ★Young Research Paper Award★\nY. Marca, H. Aguirre, S. Zapotecas, A. Liefooghe, B. Derbel, S. Verel, and K. Tanaka. Pareto dominance-based MOEAs on problems with difficult pareto set topologies. In Proceedings of the Genetic and Evolutionary Computation Conference Companion (GECCO ’18). ACM, New York, NY, USA, 189-190. DOI.\nC. E. A. L. Rocha, Y. P. Marca, F. K. Schneider. Support Platform for Decision-Making in Research and Technological Development in Public Health. ESPACIOS (CARACAS), v. 39, p. 14-26, 2018. (link)"
  },
  {
    "objectID": "old_content/publication.html#publications-in-portuguese",
    "href": "old_content/publication.html#publications-in-portuguese",
    "title": "Yuri Marca’s Homepage",
    "section": "Publications in Portuguese",
    "text": "Publications in Portuguese\n\nY. P. Marca, S. Scholze. Proposta de Substituição da Comunicação GSM em Smart Grids por Rádios de Longo Alcance. XXXIII Simpósio Brasileiro de Telecomunicações, 2015, Juiz de Fora, MG. Anais Completo da Programação Técnica, 2015.\nY. P. Marca, C. E. A. L. Rocha, B. Schneider Jr , F. K. Schneider. Plataforma de Apoio ao Processo Decisório em Pesquisa e Desenvolvimento Tecnológico em Saúde. Congresso Brasileiro de Engenharia Biomédica, 2012, Porto de Galinhas. ANAIS - CBEB 2012, 2012. (pdf)\nM. P. Krause, D. M. Nakato, Y. P. Marca, F. K. Schneider. Gerenciamento do Controle da Glicemia Utilizando um Aplicativo para Celular. Congresso Brasileiro de Engenharia Biomédica, 2012, Porto de Galinhas. ANAIS - CBEB 2012, 2012."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "👋 Hi there, I’m Yuri Marca",
    "section": "",
    "text": "o I am a Data Scientist with a strong background in machine learning, optimization, and MLOps, focused on developing scalable AI solutions and deploying models into production. I hold a Bachelor’s in Electronic Engineering and a Master’s in Information Systems, with international academic experience across Japan, the United Kingdom, and Canada. This diverse background has equipped me with strong analytical skills, the ability to quickly understand and implement state-of-the-art research, and a scientific approach to solving complex industry challenges.\nIn my industry experience, I have worked extensively with machine learning and deep learning frameworks to train and fine-tune models for predictive modeling, anomaly detection, and computer vision. My expertise in MLOps enables me to design and implement end-to-end pipelines that ensure reproducibility and scalability. Leveraging tools like MLflow, Docker, and cloud platforms like AWS and DigitalOcean, I have successfully deployed machine learning models and integrated them into production via API services.\nWith a strong statistical analysis and optimization foundation, I am particularly interested in Generative AI and LLMs, actively exploring their potential in real-world applications. I am eager to contribute to cutting-edge AI solutions and look forward to collaborating with professionals and organizations driving AI innovation in production environments."
  },
  {
    "objectID": "index.html#technical-expertise",
    "href": "index.html#technical-expertise",
    "title": "👋 Hi there, I’m Yuri Marca",
    "section": "🛠️ Technical Expertise",
    "text": "🛠️ Technical Expertise\n\nOperating Systems:  Linux (Fedora, Ubuntu),  Windows.\nProgramming Languages:  Python,  C,  C++,  Bash (Unix Shell).\nMachine Learning Frameworks:  PyTorch,  scikit-learn,  XGBoost, FastAI.\nMLOps & Deployment Tools:  Docker, MLflow, Hydra,  Git,  CI/CD (GitLab, GitHub Actions),  FastAPI.\nData Engineering & Pipelines:  NumPy,  Pandas, Dask,  PostgreSQL, Redshift, InfluxDB.\nCloud Platforms:  AWS,  DigitalOcean.\nOptimization: Bayesian Optimization, Multi-objective Evolutionary Algorithms."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "👋 Hi there, I’m Yuri Marca",
    "section": "🌟 Projects",
    "text": "🌟 Projects\n\nMusic Genre Classification: Built a machine learning model to classify music genres based on audio features.\nML Pipeline for Short-Term Rental Prices: Designed and implemented a full ML pipeline to predict short-term rental prices, integrating data ingestion, preprocessing, model training, and deployment using industry best practices.\nImage Description using OpenAI: Developed an image captioning model leveraging OpenAI’s API to generate meaningful descriptions for images.\nOCBA-MCTS: Reproduced results from the OCBA-MCTS paper, focusing on optimization in Monte Carlo Tree Search through Optimal Computing Budget Allocation algorithm."
  }
]